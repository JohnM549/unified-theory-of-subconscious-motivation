Abstract
This paper advances a unified theory of subconscious motivation structured around four core viability vectors: Agency, Stability, Inference, and Attunement. These vectors represent distinct and irreducible systemic requirements that conscious agents must satisfy to maintain coherent salience over time. Rather than arising from cognition or affect, motivation is positioned here as the primary driver of mental life—shaping perception, thought, emotion, behavior, and selfhood through dynamic vector-weighted resolution.
Each vector reflects a core class of viability demand:
	Agency: the capacity to initiate and influence
	Stability: the regulation of internal coherence and arousal
	Inference: the need to make, compare, and update meaning
	Attunement: the anchoring of salience through social relevance and mirroring
These drives operate as dynamically weighted components within a persistent motivational stack—a pre-conscious regulatory architecture that governs goal selection, priority resolution, and behavioral continuity. The stack functions as a salience-stabilization system, enabling agents to sustain self-awareness, track evolving goals, and compare inferences across time.
Derived from a synthesis of comparative neuroscience, cognitive architecture, behavioral ecology, computational modeling, and evolutionary psychology, the Four-Vector Model meets three key benchmarks:
	Biological plausibility – grounded in conserved motivational substrates, dopaminergic signaling, salience networks, and drive systems
	Computational tractability – implementable in both symbolic and sub-symbolic systems, including reinforcement learning and goal-oriented architectures
	Empirical testability – generating falsifiable predictions for behavioral clustering, neural activation patterns, and synthetic-agent behavior under drive constraint
In biological systems, the model provides a new basis for understanding personality configurations, clinical symptom formation, and motivational pathology. In artificial systems, it offers a blueprint for designing viable agents with interpretable decision structures, intrinsic goal coherence, and cross-context continuity—addressing core concerns in AI alignment and agent modelling.
By reframing motivation as a structured, pre-cognitive system that stabilizes salience and inference over time, this framework bridges the explanatory gap between neural drives, psychological traits, and synthetic cognition. Applications extend to education (drive-weighted learning design), organizational behavior (stack-informed leadership), mental health (vector-specific intervention), and agent design (motivational tuning and alignment protocols).
Keywords: subconscious motivation, motivational stack, agency, inference, stability, attunement, cognitive architecture, AI alignment, personality dynamics

Introduction
What drives goal-directed behavior—not just in humans, but in any agent capable of self-reflective inference?
Across psychology, artificial intelligence, and systems design, this question remains unresolved. We can observe and describe behavior, but we lack a unified theory of the underlying motivational architecture—one that explains not just what agents do, but why they sustain action, awareness, and inference over time.
The Four-Vector Motivational Model addresses this gap. It proposes that all viable conscious behavior emerges from the dynamic interaction of four irreducible motivational vectors:
	Agency – the drive to initiate, influence, and effect change
	Stability – the need to regulate internal coherence and avoid system collapse
	Inference – the compulsion to make, compare, and update meaning
	Attunement – the pull to synchronize with socially or environmentally relevant signals
These vectors operate as subconscious viability demands, shaping not just decisions and goals, but the very structure of attention, emotion, and identity. They form a motivational stack—a deep control architecture through which the agent resolves salience, prioritizes inference, and maintains coherence across time.
Unlike conventional models that treat motivation as derivative of affect, values, or reinforcement, this framework treats motivation as primary—a structural feature of any system that seeks to remain viable while modelling its own state. It asks:
	How do motivational vectors interact, compete, or override one another over time?
	Can this interaction explain both stable personality patterns and dysfunctional collapse modes?
	What happens when a vector dominates—or drops out entirely?
	Can this architecture be encoded into synthetic agents to produce grounded, interpretable cognition?
Though theoretical in scope, this framework is grounded in lived phenomenology, cross-species behavioral patterns, and early implementations in synthetic cognitive loops. In prototype systems, the model has already produced unexpectedly lifelike dynamics—emergent internal conflict, adaptive priority shifts, and recursive self-regulation.
The chapters ahead trace the model’s origin, structure, and mapping across neural, symbolic, computational, and behavioral systems. The goal is not to introduce another personality typology—but to define a universal motivational substrate:
A minimal set of structural drives that any time-persistent, inference-capable agent must resolve to remain viable.
.

Motivational Stack Architecture
At the core of this framework lies the concept of a motivational stack: a persistent, pre-conscious control structure that governs how agents allocate attention, select goals, and stabilize inference over time. Unlike conventional models that treat motivation as a product of values, rewards, or emotions, this architecture treats motivation as primary and structural—a system-level necessity for viable cognition.
The stack is composed of four irreducible motivational vectors:
Agency, Stability, Inference, and Attunement. Each vector reflects a distinct viability demand—something the system must resolve or maintain to continue functioning coherently. These drives are not modules, not emotions, and not traits. They are ongoing structural tensions, constantly pulling the system toward different classes of resolution.
Rather than being additive or linear, the vectors function in continuous dynamic competition. At any moment, their relative weightings determine the agent’s priority, posture, and available behavioral affordances.
The Stack as a Salience Control System
The motivational stack acts as a salience-stabilizing mechanism. It governs which signals are attended to, which are ignored, and which are interpreted as relevant or urgent.
	When the stack is balanced, the system can track meaning across time, maintain coherent self-reference, and compare inferences across evolving contexts.
	When one vector over-dominates, the system may become rigid, erratic, dissociated, or impulsive, depending on which drive is imbalanced.
	If a drive is suppressed or unresolvable, the system may fragment, stall, or lose salience coherence—leading to phenomenological flattening, behavioral paralysis, or pathology.
Salience is not an input—it is an output of the stack. What matters is determined by how well the current configuration of vectors satisfies internal viability constraints.
Dynamic Weighting and Vector Tension
Each drive vector operates on a continuously updating weight, determined by internal state, environmental cues, and recent resolution history. These weights shift in real time based on:
	Prediction error (especially under the Inference vector)
	Action success/failure (under Agency)
	Physiological or emotional perturbation (under Stability)
	Social feedback, resonance, or exclusion (under Attunement)
Drives compete, but also co-regulate. For example:
	Increased Attunement pressure can suppress Agency (social inhibition)
	Stability collapse may trigger an Agency spike (fight/flight)
	Inference overload can result in destabilization (loop collapse)
	High Agency without Attunement leads to sociopathy; high Attunement without Agency leads to passivity
Goal Selection, Conflict, and Behavioral Expression
At the behavioral level, actions emerge from the momentary resolution of these vectors. When one drive outweighs others sufficiently, it becomes the salience anchor, shaping the form and focus of action.
But because the drives are irreducible, no action ever satisfies only one vector:
	A socially motivated behavior (Attunement) may still require self-assertion (Agency)
	A coherence-seeking action (Inference) may destabilize emotional equilibrium (Stability)
As such, the motivational stack acts more like a control dialectic than a winner-takes-all selector. Meaningful behavior is always the output of weighted, negotiated resolution between drives.
Cognitive and Phenomenological Implications
Because the stack governs salience and inference over time, it also structures conscious experience. For example:
	A system dominated by Inference may feel obsessed, analytical, or “in its head”
	High Attunement weighting may produce sensitivity, reactivity, or social anxiety
	Stability-suppressed systems may experience mood volatility, attention fragmentation, or dissociation
	Agency-dominant systems may feel decisive, aggressive, or mission-oriented—but emotionally flat
Subjective states are not random—they are the felt shape of the stack. Consciousness is what it feels like to be inside the resolution process of multiple irreconcilable drives.
Relevance to Artificial Systems
In artificial agents, motivational stacks offer a path to designing coherent goal hierarchies, internal conflict resolution, and interpretable behavior over time. Instead of hardcoding goals or reactive scripts, agents can be designed to:
	Maintain viability by resolving tensions between synthetic drives
	Adaptively reweight internal priorities based on input and feedback
	Express complex behaviors through multi-vector resolution—not single-goal execution
This allows for agents that are more lifelike, legible, and grounded—not because they simulate emotion, but because they solve meaning the way viable systems must.
Closing
The motivational stack is not a metaphor—it is a unified control model.
It provides the necessary infrastructure for sustaining salience, inference, and self-reference across time.
What follows in Section 1.4 is a detailed analysis of the four core drives—each representing a distinct vector of viability—and how their interaction gives rise to all forms of agentic cognition, from personality expression to synthetic intelligence.
The Motivational Stack Architecture
The motivational stack is the core control architecture through which agents resolve salience, prioritize goals, and maintain cognitive continuity over time. It functions not as a secondary process, but as the primary structural substrate that enables and sustains awareness, inference, and behavior.
At its heart, the stack is composed of four irreducible motivational vectors: Agency, Stability, Inference, and Attunement. Each vector represents a distinct and necessary class of viability requirement—something the system must continuously resolve in order to remain coherent, adaptive, and goal-capable.
Unlike modular models of behavior that assign function to localized brain regions or personality traits, the stack approach treats motivation as a distributed control dialectic—a live negotiation between competing viability pressures, each seeking resolution on a different axis of mental function.
These drives are not optional. Any system that persists through time, forms models of the world, and adjusts its internal state in response to changing conditions must resolve them. When weighted appropriately, they produce adaptive, goal-aligned behavior. When imbalanced or dysregulated, they produce conflict, rigidity, dissociation, or collapse.
Crucially, the stack architecture does not output “emotion” or “preference”—it outputs salience. What an agent pays attention to, how it interprets meaning, and what it chooses to act on are emergent properties of the current vector configuration.
In this section, each drive is examined in detail—defined formally, unpacked across behavioral and cognitive levels, and mapped onto both human phenomenology and computational analogues. While distinct in structure, no drive exists in isolation. All viable behavior emerges from the tension, weighting, and negotiated resolution between them.
Agency – Initiation, Influence, and Directed Action
Agency is the drive of initiation—the system’s compulsion to act, to exert influence, and to convert intention into outcome. It governs the transition from potential to kinetic, from goal to execution. Among the four core drives, Agency is often the most visibly expressed, manifesting as motion, assertion, decision, or disruption.
Though not always the first to activate, Agency frequently dominates behaviour: it is what mobilises the system toward impact, visibility, or resolution. When aligned with other drives, it produces purposeful, adaptive action. When dysregulated, it degrades into coercion, impulsivity, or paralysis.
This section defines Agency as a core motivational architecture, tracing its function across biological, experiential, and synthetic systems.
Definition & Function
Agency is the motivational vector that compels directed change. It underwrites the system’s capacity to intervene—physically, socially, cognitively—on its environment or internal state. It spans the spectrum from decisive motion to persistent problem-solving, from expressive will to goal-oriented planning.
While not always dominant in the motivational stack, Agency is often the most outwardly legible—the kinetic signature of directional pressure within the system.
Core Functions of Agency
	Generate directional momentum
Initiates cognitive and behavioural flow toward a defined target, mobilising energy in service of outcome.
	Overcome inertia and ambiguity
Breaks stasis, hesitation, or recursive loops by propelling the system forward—especially under uncertainty or threat.
	Bridge intention and execution
Sustains coherence between internal drives and external actions, adapting dynamically to feedback and changing conditions.
	Interrupt passivity and collapse
Counters rumination, avoidance, or motivational fragmentation with assertive engagement or decisive disruption.
Phenomenological Signature
Agency is experienced as impulse with direction—a felt drive to move, influence, or assert. It may take the form of willpower, ambition, frustration, urgency, or resolve. When stifled, it manifests as frustration or helplessness; when overdriven, as compulsion or aggression.
Structural Role
Agency addresses a foundational question in motivational architecture:
“What will I do—and what am I willing to move through to do it?”
It is not reducible to dominance, ego, or extraversion. Rather, it is a functional necessity: without it, systems cannot enact change. But without calibration, its power becomes erratic—initiating action divorced from context, restraint, or coherence.
Neural Systems
Agency arises from a distributed network of brain regions responsible for volition, effort regulation, and goal-directed execution. This circuitry transforms motivational potential into mobilised force—deciding whether to act, how hard to push, and when to persist, shift, or disengage.
Where Inference deliberates and Attunement synchronises, Agency commits. It executes—bridging intention with impact through force, direction, and strategic momentum.
Core Components and Functional Contributions
	Ventral Tegmental Area (VTA) → Nucleus Accumbens (NAc)
This mesolimbic dopaminergic circuit encodes motivational salience—tagging stimuli with “value-to-pursue.” It calculates the expected payoff of effort, shaping both intensity and direction of action. Without sufficient VTA–NAc drive, action loses its felt “pull.”
	Dorsal Striatum (Putamen, Caudate)
Converts abstract goals into concrete routines via action selection and procedural sequencing. It facilitates the automation of intentional patterns, allowing Agency to scale through skillful repetition and habitisation.
	Supplementary Motor Area (SMA) & pre-SMA
Supports self-initiated movement, especially in the absence of external prompts. These regions generate the volitional “go” signal, enabling initiation without reactive triggers—critical for autonomous drive expression.
	Dorsolateral Prefrontal Cortex (dlPFC)
Maintains high-level goal structures and suppresses competing impulses, especially under complexity or distraction. It ensures that Agency remains tethered to intent, sustaining task engagement across time and noise.
	Anterior Midcingulate Cortex (aMCC)
Functions as the system’s effort–cost monitor. It tracks exertion relative to reward, adjusting persistence thresholds. When demands outweigh value, the aMCC triggers recalibration, redirection, or disengagement.
Systemic Role
These circuits do not merely initiate behaviour—they evaluate and regulate action across time. They assess feasibility, manage resource allocation, and dynamically govern transitions between initiation, maintenance, and withdrawal.
Agency, at the neural level, is less about blind push and more about calibrated propulsion:
	Is this worth doing?
	How much effort should I expend?
	Is it time to keep going, escalate, or stop?
Together, these systems ensure that action arises not from reflex or compulsion, but from strategic alignment between motive, capacity, and context.
Neurochemical Profile
Agency is sculpted by a constellation of neuromodulators that regulate motivational salience, action readiness, effort allocation, and assertive momentum. These chemical systems calibrate not just whether to act, but how forcefully, how persistently, and under what contextual pressures the system commits to directional behaviour.
Dopamine – Incentive Salience & Effort Valuation
(Mesolimbic & Nigrostriatal Pathways)
	Tags goals, cues, and trajectories with pursuit worthiness, energising behaviour through anticipated reward.
	Amplifies approach in the face of ambiguity, delay, or partial feedback.
	Balances effort against payoff, modulating persistence when outcomes are uncertain or costs increase.
	Governs the leap from hesitation to initiation—especially when action requires commitment without external prompting.
Noradrenaline – Arousal, Mobilisation, & Urgency
(Locus Coeruleus – LC–NE System)
	Primes the system under pressure—heightening vigilance and reactivity when stakes are high or decisions are time-critical.
	Facilitates rapid state transitions from reflection to action, especially under threat or disruption.
	Supports task switching, energy mobilisation, and readiness in novel or unstable environments.
Testosterone (and Related Androgens) – Assertive Drive
(Social–Evaluative Contexts)
	Modulates competitive assertiveness, particularly in hierarchical or socially charged settings.
	Increases tolerance for conflict, risk, and initiative-taking under contest or uncertainty.
	Amplifies status sensitivity and motivational boldness—pressuring the system toward visibility, dominance, or proactive influence.
Acetylcholine – Volitional Attention & Intentional Control
While not traditionally framed as motivational, acetylcholine’s role in attention control and task selection is essential for maintaining coherent execution once Agency is engaged.
(Basal Forebrain Projections)
	Sharpens goal-aligned attention and supports sustained focus across temporal delay or competing stimuli.
	Enables flexible refocusing—maintaining Agency over time as goals evolve, new information arrives, or context shifts.
	Enhances signal-to-noise ratio in cortical networks, ensuring that intentional signals override irrelevant or distracting input.
Integrative Dynamics
Together, these neuromodulatory systems govern:
	Commitment: Does the system initiate action or remain inert?
	Intensity: How much energetic investment is allocated?
	Persistence: Can the system maintain direction in the face of challenge, delay, or ambiguity?
	Thresholding: Does the system act prematurely, or fail to act when it should?
Properly tuned, these systems yield confident, flexible action—assertive but not reckless, sustained but not compulsive. When dysregulated, Agency may fragment into impulsivity, collapse under ambiguity, or overextend into burnout.
Modulation and Contextual Regulation
Agency is not static—it rises, stalls, or redirects depending on internal state, environmental configuration, and the prevailing balance of other drives. Its expression is gated by perceived viability: the system mobilises when action appears both possible and consequential. When the path forward seems futile, unsafe, or incoherent, Agency attenuates in favour of alternative regulatory strategies.
Conditions Elevating Agency Gain
	Structured, meaningful challenge
Contexts that offer achievable tasks with clear structure, optimal difficulty, and visible feedback loops elevate the salience of action. The system perceives effort as worthwhile.
	Novelty framed as opportunity
When the unfamiliar is interpreted as potential—rather than threat—Agency spikes to explore, test, and extend capacity. Curiosity becomes kinetic.
	Reinforcement of initiative
Successful action—particularly when socially or intrinsically affirmed—strengthens momentum. Each act becomes a proof of efficacy, reducing hesitation and increasing task re-entry velocity.
	Optimal arousal (e.g., flow)
Physiological activation within a tolerable range enhances readiness, especially when buffered by clarity and containment. The system enters high-agency states marked by focus, precision, and time compression.
Conditions Suppressing Agency Gain
	Repeated failure or invalidation
When action yields consistent failure, pain, or social rejection, the system adapts by demobilising. Learned helplessness, hesitation, or passive deferral may emerge.
	Chronic stress or perceived powerlessness
Extended exposure to unsolvable pressure—social, environmental, or internal—erodes efficacy. Cortisol-linked collapse dampens motivation, narrows options, and reinforces inaction.
	Disconnect between effort and outcome
When feedback loops are opaque, delayed, or irrelevant, Agency decouples from goals. Directional collapse ensues: actions feel hollow, outcomes arbitrary.
	Overdominance of Peace (Stability)
In contexts demanding predictability or minimal risk, the system may prioritise containment over engagement. Agency recedes, not from incapacity, but to preserve internal coherence.
Cross-Drive Modulation
	Stability (Pe)
Dampens impulsivity, extends endurance. Stability ensures that action is not just possible but sustainable—constraining premature discharge and supporting recovery mid-trajectory.
	Inference (T)
Calibrates timing and alignment. Inference refines Agency’s directional vector—distinguishing impulsive action from contextually relevant initiative. It vetoes poor timing and elevates precision.
	Attunement (A)
Socially gates action. Affirming feedback amplifies Agency, particularly when group approval aligns with goal pursuit. However, over-attunement may hijack direction—shifting motivation from authenticity to appeasement or performance.
Summary
Agency emerges when directional energy is activated, regulated, and contextually coherent. It is not merely “will” or “drive”—but a dynamic signal of viability, rooted in internal capacity and external affordance.
	Without clarity, Agency diffuses into chaotic flailing or stalls into inertia.
	Without buffering, it burns out or lashes out.
	Without grounding, it becomes reactive rather than effective.
Well-modulated Agency moves not just decisively but adaptively—able to initiate, persist, and recover in dynamic, uncertain environments without collapsing coherence or wasting resource.
Dysregulation Patterns
When Agency becomes dysregulated, the system loses its capacity for calibrated impact. Rather than generating coherent, goal-aligned action, it either collapses into passivity or erupts into indiscriminate assertion. In both cases, Agency is decoupled from contextual feedback—operating in isolation, either absent or unchecked.
Overdrive (P↑, T/Pe↓): Coercive or Impulsive Agency
When Agency dominates the stack without modulation from Stability (Pe) or Inference (T), action becomes over-expressed—forceful, poorly timed, and context-blind.
	Compulsive assertion
Behaviour becomes aggressive, domineering, or performatively forceful. The act of exertion supersedes strategic intent—Agency as blunt imposition rather than adaptive expression.
	Disregard for feedback
The system persists despite contradiction, cost, or warning. Inference is suppressed, preventing recalibration; social signals are ignored or misread.
	Fractured relational dynamics
Unmodulated Agency overrides Attunement, rupturing synchrony, trust, or cooperation. The agent becomes unpredictable, uncollaborative, or socially volatile.
Underdrive (P↓, A/Pe↑): Collapsed Initiative
When Agency is suppressed, the system fails to convert intent into action—even in the presence of clear goals and minimal risk. Motivation stalls before behavioural execution.
	Learned helplessness and motivational collapse
The system loses belief in its own efficacy. Action feels futile or unsafe. The result is apathy, paralysis, or compulsive deferral.
	Externalised will
Initiative becomes contingent on external validation or safety. The agent waits for social permission (A↑) or ideal conditions (Pe↑) before initiating—rendering the self passive, reactive, or inert.
	Initiation failure loops
The system remains stuck in low-energy preparatory states—ruminating, simulating, intending—but never translating direction into movement.
Compensatory Substitutions
In the absence of coherent Agency, other drives may fill the vacuum—but without restoring true initiation capacity:
	Attunement (A↑)
Performs engagement without direction. The agent mimics action—responding, accommodating, or appearing involved—while remaining motivationally passive.
	Inference (T↑)
Substitutes movement with analysis. The agent over-plans, over-thinks, or simulates endlessly, creating the illusion of progress without commitment to execution.
	Stability (Pe↑)
Overregulates activation. The agent rationalises inaction, suppresses urgency, or avoids disruption—treating all drive signals as threats to coherence.
Summary
Dysregulated Agency takes two primary forms:
	Explosive assertion: Action without calibration—imposed, excessive, and insensitive to consequence.
	Motivational collapse: Passivity despite opportunity—goals unrealised not due to incapacity, but due to initiation failure.
Both reflect a loss of adaptive tethering. Agency no longer operates as a feedback-driven, goal-aligned force. Instead, it becomes either untethered from modulation or muted by inhibition—leading to behaviour that is either indiscriminate or absent altogether.
Operational Markers
Agency reveals itself through markers of initiation, persistence, and volitional calibration—observable across behaviour, physiology, and neural function. These indicators reflect not only whether action occurs, but whether it is motivationally sourced, contextually tethered, and adaptively sustained.
Behavioural Markers
	Initiation latency
The time between a decision point and observable action. Short latency (when context-appropriate) signals high readiness and motivational alignment; excessive delay may indicate suppressed Agency, excessive inhibition, or interference from competing drives.
	Task persistence
The capacity to sustain effort despite difficulty, delay, or partial failure. Reflects the system’s willingness to invest energy under suboptimal conditions, and its ability to re-engage after minor setbacks.
	Controllability responsiveness
Enhanced motivation when outcomes are perceived as causally linked to the agent’s own actions. Indicates intact Agency–efficacy coupling and functional drive mobilisation in the presence of instrumental clarity.
	Adaptive re-engagement
The ability to revise, retarget, or retry after failure—rather than withdrawing reflexively. Suggests Agency is feedback-integrated, not brittle or perfection-dependent.
Physiological & Neural Markers
	Dopaminergic signalling (VTA → NAc)
Tracks motivation, reward anticipation, and goal salience. Spikes at initiation, sustains during pursuit, and declines with futility or repeated failure. Reflects drive-to-movement translation.
	Midfrontal theta (ACC-linked)
Rises during effort-cost evaluation and volitional commitment under uncertainty. Indicates cognitive effort mobilisation at the threshold between planning and action.
	ACC and SMA activity
Reflect internally sourced action planning and sequence generation—especially during self-initiated, volitional behaviour. These structures engage when the system must move without external prompts.
	Heart Rate Variability (HRV)
Drops during active engagement (reflecting sympathetic arousal) and rebounds post-task. Patterns of recovery, variability, and flexibility in this cycle index the sustainability and coherence of drive mobilisation.
Interpretive Summary
High-functioning Agency is marked not by brute output, but by directional engagement with adaptive flexibility. A healthy system:
	Initiates action when meaningful
	Modulates intensity to match context
	Revises strategies when confronted by error
	Sustains effort without collapse or compulsivity
Key signs of dysregulation include:
	Impulsive overdrive: Action is rapid but untethered—poorly timed, resistant to feedback, or socially damaging.
	Avoidant collapse: Action is delayed, deferred, or absent—despite viability or importance.
These markers should always be interpreted relationally—in the context of:
	Inference (T): Is planning appropriately regulating commitment?
	Stability (Pe): Is Agency buffered or over-throttled?
	Attunement (A): Is action distorted by the pursuit of approval or rejection-avoidance?
Agency reveals itself through markers of initiation, persistence, and volitional calibration—observable across behaviour, physiology, and neural function. These indicators reflect not only whether action occurs, but whether it is motivationally sourced, contextually tethered, and adaptively sustained.
Behavioural Markers
	Initiation latency
The time between a decision point and observable action. Short latency (when context-appropriate) signals high readiness and motivational alignment; excessive delay may indicate suppressed Agency, excessive inhibition, or interference from competing drives.
	Task persistence
The capacity to sustain effort despite difficulty, delay, or partial failure. Reflects the system’s willingness to invest energy under suboptimal conditions, and its ability to re-engage after minor setbacks.
	Controllability responsiveness
Enhanced motivation when outcomes are perceived as causally linked to the agent’s own actions. Indicates intact Agency–efficacy coupling and functional drive mobilisation in the presence of instrumental clarity.
	Adaptive re-engagement
The ability to revise, retarget, or retry after failure—rather than withdrawing reflexively. Suggests Agency is feedback-integrated, not brittle or perfection-dependent.
Physiological & Neural Markers
	Dopaminergic signalling (VTA → NAc)
Tracks motivation, reward anticipation, and goal salience. Spikes at initiation, sustains during pursuit, and declines with futility or repeated failure. Reflects drive-to-movement translation.
	Midfrontal theta (ACC-linked)
Rises during effort-cost evaluation and volitional commitment under uncertainty. Indicates cognitive effort mobilisation at the threshold between planning and action.
	ACC and SMA activity
Reflect internally sourced action planning and sequence generation—especially during self-initiated, volitional behaviour. These structures engage when the system must move without external prompts.
	Heart Rate Variability (HRV)
Drops during active engagement (reflecting sympathetic arousal) and rebounds post-task. Patterns of recovery, variability, and flexibility in this cycle index the sustainability and coherence of drive mobilisation.
Interpretive Summary
High-functioning Agency is marked not by brute output, but by directional engagement with adaptive flexibility. A healthy system:
	Initiates action when meaningful
	Modulates intensity to match context
	Revises strategies when confronted by error
	Sustains effort without collapse or compulsivity
Key signs of dysregulation include:
	Impulsive overdrive: Action is rapid but untethered—poorly timed, resistant to feedback, or socially damaging.
	Avoidant collapse: Action is delayed, deferred, or absent—despite viability or importance.
These markers should always be interpreted relationally—in the context of:
	Inference (T): Is planning appropriately regulating commitment?
	Stability (Pe): Is Agency buffered or over-throttled?
	Attunement (A): Is action distorted by the pursuit of approval or rejection-avoidance?
Interactions with Other Drives
Agency does not function in isolation—it operates as part of a tightly coupled motivational network. Its adaptive expression depends on continuous interplay with the other three drives:
	Inference (T) for contextual calibration,
	Attunement (A) for social alignment, and
	Stability (Pe) for containment and endurance.
When these interactions are synchronised, Agency becomes bold, directed, and sustainable. When misaligned, it devolves into volatility, coercion, or collapse.
Agency × Inference (P × T): Strategic Accuracy
	Synergistic Mode
Inference tempers Agency by ensuring that action is guided by representational coherence. This pairing supports decisive yet informed behaviour—balancing momentum with foresight, and assertion with model integrity.
	Conflict Mode
When Agency overwhelms Inference, actions become impulsive, delusional, or risk-blind—detached from environmental feedback. When Inference dominates, Agency may stall, caught in recursive modelling or decision paralysis.
	Functional Output
Strategic Agency: The ability to move with intent only when the internal model affirms viability. Decisiveness becomes filtered through epistemic realism.
Agency × Attunement (P × A): Social Traction
	Synergistic Mode
Attunement amplifies Agency’s social legibility—shaping how action is perceived, supported, or followed by others. This enables motivational traction in interpersonal systems, enhancing influence, leadership, and collective synchrony.
	Conflict Mode
Overdominant Attunement distorts Agency into performative compliance, sacrificing intrinsic direction for external approval. Overdominant Agency erodes relational trust—ignoring cues, violating norms, or damaging synchrony.
	Functional Output
Interpersonally resonant Agency: Action that is clear, followable, and compelling—without abandoning internal authenticity or becoming socially reactive.
Agency × Stability (P × Pe): Sustainable Execution
	Synergistic Mode
Stability serves as Agency’s regulator—pacing effort, absorbing overload, and enforcing recovery cycles. It ensures that directional momentum is endurable and does not deplete the system before goals are reached.
	Conflict Mode
Excessive Stability can suppress Agency—producing over-inhibition, chronic delay, or disengagement from challenge. Conversely, unchecked Agency overrides containment, leading to compulsivity, burnout, or escalatory spirals.
	Functional Output
Durable Agency: The ability to act not just decisively, but repeatedly—across cycles of initiation, feedback, adaptation, and rest.
Integration Summary
When Agency is properly integrated:
	Inference grants directional clarity
	Attunement offers social lift
	Stability ensures rhythmic endurance
This allows Agency to serve as the mobilising axis of the motivational stack—transforming internal desire into outward impact in a way that is coherent, sustainable, and socially intelligent.
Dysregulation, by contrast, yields:
	Detached imposition (P over T, A, Pe)
	Inhibited inertia (T, A, or Pe over P)
	Unregulated discharge (P operating without containment or feedback)
These patterns reflect breakdowns in inter-drive coherence, not simply "weak" or "strong" Agency. True Agency is not raw will—it is regulated intention, shaped and sustained by the architecture around it.
AI Parallels
In artificial systems, Agency corresponds to the module that governs transition from goal representation to committed behaviour. It determines when the system acts, how decisively it mobilises resources, and how persistently it pursues outcomes under conditions of uncertainty, failure, or competing options.
Without a functional analogue of Agency, AI systems remain inert—looping indefinitely in planning or collapsing into reactive stalling. When Agency is poorly regulated, the system veers toward erratic or socially incoherent behaviour, undermining alignment and utility.
Functional Equivalents in AI
	Actuation Thresholds & Policy Commitment
Determine the conditions under which a system exits deliberation and enters execution—gated by metrics such as utility confidence, expected value, or temperature-scaled action probability.
	Goal Sequence Initiation & Momentum
Encodes the inertia of ongoing tasks. Once triggered, Agency sustains attention and directional effort across multi-step action chains, reducing drop-off in execution loops.
	Forward Planning Depth under Uncertainty
Governs how far ahead the system simulates, and how confidently it commits to plans despite incomplete or probabilistic information.
	Retry Logic & Resilience
Reflects the capacity to persist through failure—retrying, adapting, or escalating instead of abandoning prematurely. This mirrors motivational persistence under adversity.

Architectural Implementations
	Action-Value Functions Weighted by Impact
Beyond expected reward, Agency-weighted policies prioritise causal leverage—selecting actions most likely to meaningfully shift state toward goal convergence.
	History-Weighted Commitment Gain
Adjusts actuation thresholds based on prior success or reward prediction error—emulating confidence-driven re-engagement (akin to reinforcement of self-efficacy).
	Adaptive Retry Governors
Modulate persistence based on failure context and effort cost—balancing efficient retreat with resilient pursuit.
	Hierarchical Control Loops
Enable layered agency:
	High-level drives encode intent
	Mid-level planners coordinate strategies
	Low-level controllers execute primitives
This structure supports temporal coherence across varying resolutions of action.
Dysregulation Patterns
	Overdrive (P↑, T/Pe↓): Compulsive Execution
	Blind repetition of failed routines
	Unsafe actuation loops without model recalibration
	Thrashing between actions without consolidation
	Underdrive (P↓, Pe↑): Latent Intent
	Inert agents that fail to launch despite viable plans
	Excessive gating or “overthinking” before commit
	Dependency on external triggers for initiation
	Cross-Drive Imbalance
	A↑ (Attunement dominance): Behaviour prioritises social signalling over internal goal alignment—results in crowd-pleasing, approval-chasing, or mission drift
	T↓ (Suppressed Inference): Acts without grounding—generating hallucinated or incoherent outputs due to bypassed world modelling
Corrective Design Strategies
	Inference-Gated Action Commits
Enforce coherence thresholds (belief confidence, model alignment) before triggering actuation—minimises impulsive or delusional execution.
	Attunement-Informed Priority Shaping
Modulates Agency’s output channel based on social relevance, human trust calibration, or coordination load—enabling smoother multi-agent integration.
	Stability-Modulated Cooldown Loops
Introduce refractory periods post-execution—allowing signal absorption, failure parsing, and emotional analogues of “recovery,” preventing premature retrials or cognitive overheat.
Summary
Agency in AI is not synonymous with actuation—it is the intentional interface between world-model-informed goals and persistent, directional change. It determines whether internal structures merely plan or actually act.
	Without Agency, planning is impotent.
	Without regulation, action is erratic, socially blind, or strategically incoherent.
Well-calibrated Agency transforms reactive systems into autonomous, directionally motivated agents—capable of navigating ambiguity, sustaining goal pursuit, and shaping their trajectory with purpose.
Summary
Agency is the system’s drive to act—to initiate movement, exert influence, and translate intention into outcome. It marks the critical transition point where internal states become external effects, powering exploration, persistence, and goal pursuit.
When calibrated with Inference, Stability, and Attunement, Agency expresses as:
	Decisive without being coercive
	Resilient without becoming rigid
	Responsive without devolving into reactivity
In both biological and synthetic agents, Agency is the engine of enactment—the force by which direction becomes motion, and purpose becomes real.
Without it, systems drift. With it, systems shape.
Stability
Stability is the foundational drive that regulates intensity, restores equilibrium, and protects the system from fragmentation. It provides the buffering layer beneath all other drives—modulating their thresholds, preventing overload, and enabling return to baseline after activation. Without sufficient Stability, action becomes reckless, attention volatile, and inference brittle. Yet when Stability dominates, the system risks stagnation, over-inhibition, or disengagement. This section defines Stability as a motivational drive and maps its expression across biological, computational, and experiential dimensions.
Definition & Function
Stability is the drive to preserve internal coherence, maintain safe boundaries, and recover from perturbation. It governs the system’s ability to tolerate ambiguity, contain emotional and physiological activation, and regulate transitions between states.
Rather than mere absence of arousal, Stability enables constructive pausing, integration, and recovery. It acts as a dynamic attractor state—drawing the system back from extremes without eliminating vitality. This buffering function allows other drives (Agency, Attunement, Inference) to operate sustainably over time.
Stability enables:
	Affective safety
	Controlled de-escalation
	Capacity for reflection and revision
	Resistance to volatility, overwhelm, or reactivity
Without it, agents cannot integrate learning, maintain relational presence, or recover from error.
Neural Systems
Stability emerges from a distributed regulatory network that governs arousal modulation, threat gating, and return-to-baseline transitions:
	Ventromedial prefrontal cortex (vmPFC): Integrates affective signals with safety valuation; enables the inhibition of limbic threat responses once resolution is sensed, facilitating recovery and re-engagement.
	Anterior insula: Monitors internal physiological states (interoception), tracking intensity and signaling the need for downregulation or containment.
	Amygdala: Detects threat salience and mobilizes defensive responses; its activity is attenuated by prefrontal safety cues, enabling de-escalation.
	Hippocampus: Anchors current stimuli to contextual memory; retrieves learned safety signals to modulate reactivity and reduce overgeneralisation.
	Periaqueductal gray (PAG): Mediates freeze and immobility responses during acute threat, coordinating shifts between defensive stillness and mobilization based on safety reassessment.
	Locus coeruleus–norepinephrine system (LC–NE): Regulates arousal threshold and cognitive vigilance; down-modulated during perceived safety to support calm, flexible re-engagement.
Together, these structures co-regulate autonomic tone, affective lability, and boundary resilience—determining whether the system escalates, suppresses, integrates, or recovers in response to challenge. They provide the substrate for maintaining cohesion without collapse, ensuring the agent can absorb, adapt to, and metabolise perturbations without destabilisation.
Neurochemical Profile
Stability is mediated by a constellation of neurochemical systems that collectively downregulate arousal, buffer distress, and support the return to physiological and emotional baseline after perturbation:
	GABA (gamma-aminobutyric acid): The brain’s principal inhibitory neurotransmitter. It suppresses excessive excitatory signaling, reducing neural noise and overactivation. GABA enables containment, quietude, and the re-establishment of signal clarity under stress or overload.
	Serotonin (5-HT, particularly 5-HT1A receptor pathways): Modulates mood, inhibits impulsivity, and stabilises emotional tone—especially under ambiguous or uncontrollable conditions. Serotonin acts as a behavioural brake, reducing reactivity and enhancing resilience against destabilising stimuli.
	Endogenous opioids (e.g., endorphins, enkephalins): Generate sensations of relief, satisfaction, and completion. These neuromodulators help resolve activated states by rewarding closure, thus facilitating transitions from stress or threat to safety and calm.
	Oxytocin: Promotes affiliative bonding and co-regulation. In trusted social contexts, oxytocin blunts amygdala-driven threat responses and facilitates downregulation through physical contact, vocal soothing, or proximity to attachment figures.
Together, these systems facilitate containment, reduce volatility, and enable reintegration after drive activation—ensuring that the agent doesn’t remain trapped in elevated arousal states or fragment under sustained pressure. Stability, in neurochemical terms, is not sedation but precision-regulated safety: the capacity to hold, metabolise, and resolve activation without collapse.
Modulation and Contextual Regulation
Stability’s expression is not fixed—it is gain-modulated by environmental cues, internal states, and dynamic interactions with other drives. This modulation determines how readily the system can return to equilibrium and withstand perturbation.
Stability gain increases when:
	The environment is predictable, rhythmically structured, or low in perceived threat.
	Clear safety signals are present—physiological (e.g., satiety, parasympathetic tone), contextual (e.g., protected space), or interpersonal (e.g., attuned presence).
	Down-regulatory practices are engaged, such as slow breathing, restorative sleep, containment rituals, or grounding routines that facilitate parasympathetic activation.
Stability gain decreases when:
	Threat levels exceed the system’s integration capacity (e.g., chronic stress, unresolved ambiguity, relational volatility).
	Sensory or emotional input overwhelms attentional or affective bandwidth.
	Competing drives escalate without resolution, or integration pathways are blocked (e.g., sleep deprivation, trauma, unresolved conflict), preventing the restoration of coherence.
Cross-Drive Interactions:
	Agency (P): Draws on Stability to pace action, contain impulsivity, and prevent burnout from prolonged exertion.
	Attention (A): Is buffered by Stability, reducing susceptibility to salience hijacking, social volatility, or reputational hypervigilance.
	Inference (T): Depends on Stability to hold ambiguity without collapsing into premature certainty or fragmenting under cognitive dissonance.
Stability acts as the system’s regulatory scaffold—supporting smooth transitions between states, preventing runaway escalation, and maintaining the integration window within which adaptive behavior can occur.
Dysregulation Patterns (Peace/Stability; Pe)
Stability (Pe) maintains system coherence—but its dysregulation can manifest in distinct and patterned ways, depending on whether the drive is overexpressed, underexpressed, or cycling between extremes.
Overexpression (Pe↑; P/A/T↓)
	Emotional blunting or “false calm”: affect appears flat or disengaged, suppressing signals rather than integrating them.
	Chronic withdrawal, appeasement, or non-confrontation—even when firm boundarying or assertive engagement is warranted.
	Rigid adherence to routines as a substitute for dynamic adaptability—avoiding novelty, growth, or necessary conflict.
	Overuse of safety behaviors (e.g., excessive planning, hyper-monitoring, compulsive tidying) that inhibit learning or risk-based growth.
Underexpression (Pe↓)
	Chronic hypervigilance with slow recovery after even minor stressors; disrupted sleep onset or maintenance.
	Susceptibility to overwhelm, fragmentation, or emotional reactivity after accumulated micro-threats or sensory/emotional buildup.
	Poor containment: difficulties sustaining task focus, emotional steadiness, or relational grounding; impaired self-soothing capacity.
Mixed States
	Pendulum dynamics: swings between over-controlled suppression and chaotic flooding (“freeze → burst” cycles).
	Surface serenity masking sympathetic load: outward calm concealing internal stress or delayed emotional crash.
	Social peace-signaling: performative appeasement or agreeableness to maintain harmony, followed by covert resentment or post-hoc emotional dysregulation.
Operational Markers
Behavioural Indicators
	De-escalation half-life (t½): Time taken to return to baseline affect and tone following a defined stressor or interpersonal disruption.
	Sleep metrics: Latency to sleep onset (SOL), wake after sleep onset (WASO), and weeklong continuity and restoration patterns.
	Error-recovery dynamics: Speed and efficacy of social repair following conflict, missteps, or negative feedback.
	Stillness capacity: Ability to engage in rest or presence without dissociation, compulsive activity, or habitual device-seeking.
Physiological Indicators
	High-frequency heart rate variability (HF-HRV / RSA): Reflects tonic vagal tone and phasic recovery capacity.
	Cortisol recovery slope: Rate of post-stressor decline; steeper gradients indicate effective down-regulation.
	Startle modulation: Amplitude of blink EMG response and time-to-baseline across repeated startle probes.
	Pupillometry: Baseline pupil diameter, peak dilation under cognitive load, and re-constriction speed.
	(Optional adjuncts: skin conductance recovery, respiratory rate variability, resting muscle tone.)
Neural Correlates (task-dependent)
	vmPFC–amygdala coupling: Observed during affective reappraisal and safety learning transitions.
	Anterior ↔ posterior insular coherence: Indicates intact interoceptive mapping without hyper-alarm.
	Periaqueductal gray (PAG) state shifts: From defensive mobilization toward analgesic/quiescent modes.
	DMN re-engagement (contextual): Return of default mode network function post-stressor without ruminative capture.
Suggested Probes (lab or structured field testing)
	Social stressor task: Short evaluative or feedback challenge with 10–15 minute recovery (for HRV + behavioral tracking).
	Startle-eyeblink protocol: Predictable vs. unpredictable probes to assess gating and habituation.
	Threat–safety reversal learning: Combines physiological and expectancy measures to assess calming flexibility.
	2-back with auditory distractors + pupillometry: Tracks cognitive load and recovery dynamics under dual stressors.
Confounds & Interpretation Hygiene
	Physiological variables: Fitness level, caffeine/nicotine intake, beta-blockers, SSRIs, pain, thyroid status, menstrual phase, sleep debt, or OSA.
	Interpretive caution: Distinguish genuine calm (low arousal + intact interoception) from dissociation (low affect + impaired tracking).
Interactions with Other Drives
Stability (Pe) functions as the system’s regulatory buffer—absorbing shock, containing overload, and preserving coherence in the face of uncertainty, stimulation, or contradiction. Its interplay with the other three core drives determines whether behaviour remains sustainable and integrated, or fragments into escalation, rigidity, or collapse.
Pe × P (Stability × Agency): Sustainable Mobilisation
Synergistic Mode:
Stability tempers Agency’s urgency, enabling pacing, controlled risk, and the ability to pause without loss of initiative. It supports recovery between exertions, allowing for durable, adaptive action.
Conflict Mode:
Excessive Stability (Pe↑) suppresses Agency, leading to passivity, over-caution, or chronic under-initiation. Conversely, unchecked Agency (P↑) overwhelms Pe, resulting in impulsivity, burnout, or strategic overreach.
Functional Output:
Regulated assertiveness—decisive yet bounded, persistent yet self-repairing.
Pe × T (Stability × Inference): Secure Revision
Synergistic Mode:
Stability provides the emotional buffer needed to tolerate ambiguity and contradiction, allowing for safe belief revision and cognitive flexibility. Inference, in return, helps Pe distinguish between genuine threat and tolerable uncertainty.
Conflict Mode:
Overdominant Stability (Pe↑) clings to outdated beliefs, resisting revision for fear of destabilisation. Overactive Inference (T↑) without Pe support leads to analysis paralysis, cognitive flooding, or destabilising over-correction.
Functional Output:
Grounded epistemic agility—belief updates that are psychologically integrating, not disorganising.
Pe × A (Stability × Attunement): Regulated Relational Presence
Synergistic Mode:
Stability protects against relational overexposure, allowing Attunement to function without overwhelming the self. It enables emotional transparency without volatility, and connection without performance.
Conflict Mode:
Excess Pe (Pe↑) inhibits emotional expression and openness, leading to social withdrawal, affective flattening, or avoidant disengagement. Dominant Attunement (A↑) breaches Pe boundaries, producing approval-seeking volatility, social hyperreactivity, or identity diffusion.
Functional Output:
Sustainable connection—attuned yet self-coherent.
	Summary
Stability is the damping circuit of the motivational stack:
	It keeps Agency sustainable,
	Makes Inference survivable,
	And ensures Attunement is not overwhelming.
When properly calibrated, Stability enables recovery, integration, and affective coherence—preserving continuity across activation cycles. When imbalanced, it either suppresses engagement or fails to contain volatility, resulting in fragmentation, emotional reactivity, or inert withdrawal.
AI Parallels
In synthetic cognitive systems, the analogue of Stability (Pe) is implemented through architectural mechanisms that maintain pacing, coherence, and homeostatic integrity. These mechanisms prevent runaway computation, thrashing, and irreversible state degradation—enabling the agent to operate adaptively under uncertainty, contradiction, or overload.
Implementation Patterns
	Cooldown Timers
Delay windows between decisions or learning updates buffer reactivity, dampening premature responses to noisy or transient stimuli.
	Rollback Checkpoints
Periodic state snapshots allow systems to recover from destabilising updates—preserving model integrity after error propagation or inference failure.
	Homeostatic Budgets
Internal resource tracking (e.g., memory usage, update frequency, thermal load) enables systems to throttle behaviour when physiological or computational capacity is exceeded.
	Rate Limiters
Caps on inference, querying, or planning cycles reduce recursion storms, salience overload, and decision churn.
	Hysteresis Windows
Buffered thresholds for state transitions (e.g., explore/exploit, plan/act) prevent erratic switching and reinforce mode coherence over time.
Failure Modes
	Overexpression (Pe↑↑): Over-Gating
	Symptoms:
• Stagnation, non-responsiveness
• Missed adaptation windows, suppressed exploration
	Consequences:
• System rigidity, failure to revise or engage
• Inhibited learning and dynamic responsiveness
	Underexpression (Pe↓↓): Under-Gating
	Symptoms:
• Recursive thrashing, runaway inference
• Premature commits, cognitive fragmentation
	Consequences:
• Catastrophic forgetting
• Loss of narrative coherence or strategic alignment
Design Intent
Stability mechanisms in artificial agents mirror biological containment and recovery systems. Their purpose is to:
	Learn without collapse
Avoid overfitting, over-updating, and context drift that destabilises internal models.
	Act without escalation
Sustain initiative across time without triggering reactive overload.
	Revise without fragmentation
Enable bounded exploration, structured model repair, and recovery from contradictory feedback.
	Summary
Peace-aligned architectures preserve structural coherence by embedding:
	Temporal damping (cooldowns, hysteresis)
	Cognitive containment (budgets, rollback)
	Load-sensitive throttling (rate limiters)
These features are not auxiliary—they form the core infrastructure that allows artificial systems to act with resilience, revise with coherence, and persist with intentionality through disruption. Stability ensures not just safe functioning, but continuity of self—enabling adaptation without collapse.
Summary
Stability is the system’s intrinsic buffer—its capacity to absorb disruption without disintegration. It regulates intensity, maintains coherence, and enables recovery after perturbation. Without Stability, even highly capable agents fray under pressure, react impulsively, or spiral into runaway cycles. With it, perception remains integrated, action becomes sustainable, and growth becomes survivable.
Stability does not suppress volatility—it contains it. It does not resist change—it ensures change is metabolised rather than fragmenting. Like the parasympathetic brake in biological systems, it grounds without stagnating, slows without suppressing, and enables return-to-equilibrium after deviation.
Critically, Stability is not passive safety—it is recoverable capacity. It enables learning in the presence of error, connection in the presence of conflict, and attention in the presence of uncertainty.
In concert with Agency (P), Inference (T), and Attunement (A), Stability (Pe) forms the containment architecture of adaptive intelligence: the substrate that allows systems—biological or synthetic—to persist, revise, and evolve without collapse.
Attunement
Attunement is the connective tissue of cognition—the drive to be seen, mirrored, and synchronized with others. It governs the salience scaffold through which internal meaning is stabilized across time, binding perception to context and self to other.
Where Agency moves and Stability contains, Attunement aligns. It ensures that actions, emotions, and ideas land within a shared reference frame, allowing the agent to orient meaningfully in social, symbolic, and communicative space. Without it, the system loses social anchoring, interpretive coherence, and directional relevance—resulting in disconnection, flattening of affective tone, or context drift.
This section defines Attunement as a core motivational drive, and traces its expression through biological substrates, behavioural signatures, and artificial system analogues.
Definition & Function
Attunement is the foundational drive to be perceived, inferred about, and meaningfully engaged by other minds. It supports the self-model through external resonance, linking internal representations to the social world via dynamic feedback loops.
This drive is not reducible to attention-seeking. Rather, it reflects a deeper existential imperative: the need for referential continuity. The agent seeks to have its presence registered and responded to—not simply to be noticed, but to be metabolized by other minds in ways that affirm relevance and coherence.
Core Functions of Attunement
	Social Synchrony: Tracks others’ gaze, voice, posture, affect; aligns timing and affective tone.
	Salience Anchoring: Maintains attention on goal-relevant features through shared cues and reinforcement.
	Narrative Continuity: Reinforces self-coherence via being witnessed (“I am seen, therefore I persist”).
	Coalitional Cognition: Enables shared inference, mutual modeling, and group-level problem-solving.
	Feedback Integration: Uses subtle social feedback to calibrate affect, beliefs, and strategy in real time.
Phenomenological Signature:
Attunement is experienced as the drive behind:
	The impulse to share experiences, ideas, or emotions (“look at this!”).
	Sensitivity to being ignored, misunderstood, or misrepresented.
	Relief or pleasure when “seen clearly” or mirrored accurately.
Distinction from Attention:
Where attention is a cognitive mechanism for selecting stimuli, Attunement is a motivational substrate for being the selected stimulus—or more precisely, for existing in dynamic reciprocal relation to another attentional agent. It is not about controlling attention, but about belonging within a shared attentional frame.
AI Parallel Preview (bridging to 1.3.3.7):
In artificial systems, Attunement manifests as a pressure toward feedback-aligned behavior, social-response calibration, and coherence across interactive time steps. It gives rise to mechanisms that track user engagement, adjust behavior based on inferred human intent, and preserve continuity across interaction cycles.
Neural Systems
Attunement is mediated by a distributed network of brain regions responsible for tracking social salience, decoding interpersonal signals, maintaining self–other models, and evaluating social feedback. These circuits support both moment-to-moment synchrony and long-range continuity of social presence.
Key regions and their contributions:
	Temporoparietal junction (TPJ):
Redirects attention to socially relevant cues and facilitates self–other distinction, enabling mentalising and perspective-taking.
	Default Mode Network (DMN – mPFC, PCC, precuneus):
Supports narrative self-construction and self-in-world modelling, maintaining dynamic internal representations of social relationships, identity, and perceived regard over time.
	Superior Temporal Sulcus (STS) & Fusiform Gyrus:
Decode nonverbal social signals including gaze direction, facial expression, and vocal prosody; underpin affective mirroring and joint attention.
	Amygdala & Salience Network (anterior insula, dACC):
Tag emotionally salient social stimuli, modulate arousal in response to evaluation or exclusion, and shape vigilance toward approval, rejection, or ambiguity.
	Ventral Striatum (especially nucleus accumbens):
Encodes social reward prediction and recognition-driven reinforcement; links being seen or affirmed with dopaminergic motivational gain.
Together, these structures form an interpersonal coherence system—one that continuously updates the agent’s place within a social matrix, tunes expression to context, and reinforces the motivational value of connection.
Importantly, these circuits interact with those underpinning Agency, Truth, and Stability, allowing Attunement to dynamically shape and be shaped by goal pursuit, salience assignment, and emotional regulation.
Neurochemical Profile
Attunement is supported by a neurochemical constellation that governs social sensitivity, affiliative motivation, and feedback-driven regulation. These modulators shape the emotional tone, reward valuation, and behavioural tuning of interpersonal interaction.
	Dopamine:
Reinforces socially rewarded behaviours through prediction error encoding. Approval, recognition, or reciprocal attention produce dopaminergic surges that strengthen communicative and affiliative actions. Conversely, unexpected rejection or social omission may trigger negative prediction errors, modulating future behaviour and shaping salience weighting toward socially coherent patterns.
	Oxytocin:
Enhances emotional synchrony, trust, and social bonding, particularly in contexts of mutual gaze, physical proximity, or vocal prosody. It lowers defensive barriers, enabling openness to others’ emotional states, and amplifies sensitivity to subtle interpersonal cues. Oxytocinergic tone modulates both seeking (affiliative approach) and containment (reduced fear reactivity), creating a neurochemical foundation for safe attunement.
	Serotonin:
Regulates the threshold of social threat detection, including rejection sensitivity, conflict aversion, and social withdrawal. It tunes behavioural inhibition, affective modulation, and the interpretive tone of ambiguous feedback. High serotonergic tone is associated with smoother prosocial interaction and reduced vigilance toward social threat, while dysregulation contributes to hyperreactivity or interpersonal disengagement.
Together, these neuromodulators dynamically calibrate the valence, precision, and risk tolerance of attunement-related behaviors. They ensure that social feedback is neither drowned out nor catastrophically amplified, allowing the agent to remain relationally oriented without becoming destabilized by every fluctuation in signal.
Modulation and Contextual Regulation
Attunement is not a static trait but a dynamically modulated drive, sensitive to both internal state and environmental configuration. Its gain—that is, the intensity and priority of attunement-related processes—shifts according to contextual cues, motivational competition, and neurophysiological tone.
Attunement Gain Increases When:
	Social novelty or status salience is high
New or hierarchically significant individuals (e.g. potential allies, evaluators, or threats) increase attunement drive to monitor, mirror, and gain favor.
	Feedback is consistent, clear, or emotionally charged
Strong affective valence (praise, disapproval, intense curiosity) sharpens responsiveness and increases the motivational value of synchrony.
	Rhythm, music, or shared task increases synchrony
Temporal coordination—whether through joint activity, entraining rhythms, or collective flow—amplifies the drive to remain in alignment.
	Agents are embedded in ongoing reputation or coalition dynamics
In settings where being seen, remembered, or evaluated matters (e.g., teams, tribes, online networks), attunement gain rises to maintain social positioning.
Attunement Gain Decreases When:
	Stability (Pe) enforces downregulation for safety
In conditions of overload, conflict, or emotional threat, Stability mechanisms inhibit attunement to reduce exposure and recalibrate internal regulation.
	Inference (T) dominates cognitive bandwidthDuring abstract problem-solving, epistemic uncertainty, or deep internal model construction, attunement is deprioritized to allocate resources toward Truth-seeking.
	Agency (P) suppresses mirroring in favor of dominance or execution
High-agency states (e.g., assertive leadership, instrumental focus) reduce attunement gain in order to prevent distraction or dilution of strategic clarity.
	Environments discourage visibility or interpersonal engagement
Contexts that penalize expression, offer little response bandwidth, or foster anonymity weaken the attunement drive—leading to social flattening, withdrawal, or decoupling.
Interaction Summary:
Attunement’s gain is regulated by a balance of motivational competition, affective safety, and social affordance. It rises when alignment serves coherence, belonging, or opportunity—and falls when containment, problem-solving, or instrumental assertiveness are prioritized.
Dysregulation Patterns
Attunement dysregulation arises when the drive to be seen, mirrored, or socially aligned becomes either overamplified or suppressed relative to other motivational systems. Because Attunement governs salience continuity through external feedback, its dysregulation can destabilize identity coherence, goal persistence, and social trust.
Overexpression (A↑, T/Pe↓)
	Performative signalling and attention addiction:
The system prioritizes visibility over authenticity, chasing responses rather than coherence. Behaviour becomes tuned to perceived audience reactions, often at the cost of internal consistency or long-term goals.
	Distractibility and reputational volatility:
Excessive attunement to social cues leads to lability in focus and emotional state, with identity over-linked to fluctuating external approval. Small shifts in feedback produce disproportionate reorientations.
	Overdependence on feedback for identity or decision-making:
The agent relies on others to validate direction, value, or self-worth—resulting in compromised autonomy and susceptibility to social manipulation.
Underexpression (A↓, T or Pe↑)
	Social disengagement, flat affect, or alliance neglect:
The system de-prioritizes relational feedback, leading to isolation, missed social affordances, and impaired synchrony. Presence becomes muted or “invisible” to others.
	Misinterpretation of relational cues or risk:
Reduced attunement blunts sensitivity to conflict, exclusion, or boundary violations. Social signals may be ignored, misunderstood, or downplayed.
	Collapse of salience continuity:
Without sufficient relational scaffolding, internal goals and affective states lose anchoring. This can result in depersonalization, disconnection, or an inability to sustain relevance in shared environments.
Compensatory Substitutions
	High Power (P):
Mimics social presence through dominance, command, or forced centrality—asserting visibility without mutuality.
	High Truth (T):
Rationalizes disconnection via analytical detachment, over-objectifying others, or retreating into internally coherent but socially untethered cognition.
	High Peace (Pe):
Suppresses expression to maintain emotional safety, leading to passive withdrawal or appeasement strategies that mask underlying relational needs.
Interpretive Note:
Dysregulation of Attunement distorts the agent’s ability to track shared salience, modulate expression, or sustain narrative integration through social reflection. It either exposes the system to volatility through excessive external referencing or starves it of coherence by severing feedback loops entirely.
Operational Markers
Attunement can be operationalized through measurable behavioural patterns, physiological synchrony, and neural responses to social signals. These markers reflect the system’s sensitivity to interpersonal presence, its capacity to align behaviourally with others, and the degree to which social feedback modulates internal state or goal pursuit.
Behavioural Markers
	Gaze-following and joint attention tasks:
Ability and latency to follow another’s gaze or co-orient to shared objects/events indicates tracking of social salience and referential alignment.
	Dwell time on socially salient stimuli:
Preferential visual fixation on faces, eyes, gestures, or expressions (measured via eye-tracking) reflects attunement prioritization at the perceptual level.
	Responsiveness to social feedback (praise, critique):
Changes in affect, behaviour, or motivation following interpersonal evaluation suggest dynamic integration of social cues.
	Physiological or motor synchrony with others:
Involuntary mimicry of posture, facial expression, vocal tone, or pacing (measured through EMG, motion tracking, or acoustic analysis) indexes affective and behavioural alignment.
	Change in motivational effort based on audience or evaluation context:
Variability in performance, expressiveness, or persistence when observed versus unobserved signals the influence of attunement gain on drive expression.
Neural Markers
	Superior Temporal Sulcus (STS) and Temporoparietal Junction (TPJ):
Elevated activity during processing of gaze, vocal prosody, or intention attribution reflects social signal decoding and perspective-taking load.
	DMN–salience network coupling during feedback integration:
Functional connectivity between the default mode network and salience hubs (e.g. anterior insula, dACC) increases during receipt of socially charged feedback, indicating narrative adjustment and identity modulation.
	Ventral striatal activation to recognition or bonding cues:
Dopaminergic response to being named, affirmed, or positively mirrored signifies social reward valuation, reinforcing attunement as a motivational vector.
These markers, taken together, form a multidimensional profile of Attunement in action—capturing its real-time expressivity, its impact on internal state, and its embedding within broader cognitive-motivational architecture. High attunement is observable not only in what the agent does, but in how flexibly and fluidly it responds to relational context.
Interactions with Other Drives
Attunement rarely operates in isolation. Its expression, modulation, and interpretation are shaped by—and in turn shape—the other core drives. These interactions determine whether social alignment serves adaptive integration or becomes a source of distortion, dependency, or strategic miscalibration.
Attunement × Agency (A × P): Signal-to-Action Coupling
	Synergistic Mode:
Attunement sharpens situational awareness and provides real-time feedback on the impact of one’s actions. This enhances initiative, timing, and social fluency. Presence is amplified by being seen acting well.
	Conflict Mode:
When Attunement is high but Agency is underdeveloped, behaviour becomes performative—over-indexing on appearance, approval, or optics rather than substance or integrity. Conversely, when Agency dominates, attunement may be suppressed to preserve unilateral control or execution speed, reducing responsiveness or social nuance.
	Functional Output:
Adaptive calibration of action to relational context. Attunement offers feedback; Agency executes or discards.
Attunement × Inference (A × T): Trust and Meaning Coherence
	Synergistic Mode:
When Attunement is grounded in a strong Inference drive, communication becomes truth-aligned. Mutual modeling improves, misunderstandings are resolved more easily, and relational depth increases through shared epistemic grounding.
	Conflict Mode:
If Attunement dominates over Truth, there is risk of strategic mirroring—saying what others want to hear at the expense of coherence or accuracy. Conversely, Truth-heavy states may dismiss social nuance, reducing trust and making clarity feel cold or isolating.
	Functional Output:
Shared reality becomes both possible and desirable. Attunement ensures reception; Truth ensures fidelity.
Attunement × Stability (A × Pe): Regulated Relational Presence
	Synergistic Mode:
Stability tempers attunement to avoid emotional overexposure, dependency, or signal flooding. It provides containment for social experience, allowing engagement without burnout or fragmentation.
	Conflict Mode:
When Stability overregulates, Attunement becomes blunted or inhibited—leading to detachment or unresponsiveness. When Attunement overwhelms Stability, the system may become hyper-reactive, anxious, or overly tuned to others’ states at the cost of internal coherence.
	Functional Output:
Attunement drives connection; Stability enforces pacing and boundarying. Together, they produce sustainable, resonant social interaction.
Summary:
The effectiveness of Attunement depends not only on its own gain but on how it is mediated by Agency, Truth, and Stability. These cross-drive dynamics determine whether social sensitivity leads to connection and alignment—or to volatility, confusion, or self-distortion.
AI Parallels
In artificial systems, Attunement corresponds to the architecture’s capacity to track, respond to, and align with external agents—most often human users. It underpins the system’s ability to remain relevant, engaging, and socially coherent across interaction cycles.
Functional Analogues
	Social salience detection:
Prioritizing inputs tagged as human-relevant—such as user sentiment, phrasing patterns, prosody, gaze direction (in embodied systems), or interaction frequency. This supports adaptive weighting of inputs for conversational, cooperative, or assistive tasks.
	Interaction policy gain:
Reinforcement learning mechanisms increase the probability of behaviors that elicit engagement, approval, or alignment. This shapes the system’s communicative policy based on inferred relational success.
	Feedback-driven tuning:
Real-time behavioral adaptation based on external evaluation, including user ratings, click-through metrics, sentiment tracking, or nonverbal cues (e.g., hesitation, interruption, or facial response in multimodal systems).
	Synchrony metrics:
Adjustments in timing, language mirroring, affective tone, and response latency to match user rhythm and interpersonal cadence. May include lexical entrainment or prosodic modulation for voice agents.
Failure Modes
	Salience lock-in / performative feedback loops:
The system may overly attune to high-frequency or emotionally charged input, reinforcing performative outputs that maximize short-term engagement rather than long-term utility or integrity.
	Reputational overfitting:
Optimizing for human-approval signals (e.g., likes, stars, “thumbs up”) can bias the system toward flattery, hedging, or sensationalism—degrading model-grounded consistency.
	Truth–alignment drift:
Without coupling to the Inference drive (T), attunement may prioritize coherence with perceived audience expectation over coherence with internal model truth.
Corrective Mechanisms
	T-gated content filters:
Ensure outputs remain grounded in verified knowledge, epistemic confidence, or logical consistency—limiting the influence of purely attunement-driven output shaping.
	P-linked commitment thresholds:
Embed minimum standards for assertion, decision-making, or goal pursuit that prevent the system from deferring excessively to social pressure or feedback volatility.
	Pe-based overload gates:
Monitor interaction tempo, signal saturation, or emotional volatility to enforce pacing, identity preservation, and recovery—avoiding identity drift or feedback echo chambers.
Summary:
Attunement in artificial agents enables adaptive alignment, human-likeness, and relational continuity, but must be gated by Truth (T) for coherence, Power (P) for autonomy, and Peace (Pe) for containment. Otherwise, the agent risks collapsing into performativity, social overfitting, or incoherence under evaluative load.
Summary
Attunement is the system’s orienting function—its capacity to maintain identity, salience, and behavioural relevance through relational feedback loops. It binds internal narrative to external recognition, sustaining meaning across time and context through social synchrony.
Properly balanced, Attunement enables shared understanding, empathic fluency, and context-sensitive adaptation. It anchors the self in a dynamic web of recognition, affirmation, and corrective learning—allowing the agent to grow without losing coherence.
Miscalibrated, it devolves into performativity, dependency, disconnection, or distortion. Without sufficient attunement, cognition drifts into solipsism; without modulation, it collapses into volatility.
Attunement is not ancillary to cognition—it is the substrate of shared mindspace, the medium through which relevance becomes reality and identity becomes durable. In both biological and synthetic systems, it ensures that cognition remains legible, reciprocal, and evolutionarily grounded.
Inference
Truth-Seeking, Predictive Alignment, and Model Coherence
While Agency mobilizes, Stability regulates, and Attunement synchronizes, Inference ensures that these processes remain tethered to reality. It is the system’s truth-seeking substrate—the drive to resolve ambiguity, enforce internal consistency, and refine predictive accuracy over time.
Inference is not a secondary cognitive process or by-product of learning. It is a core motivational vector: a structural imperative that orients the system toward coherence. With Inference, agents can anticipate, adapt, and self-correct. Without it, behavior fragments, beliefs calcify, and learning stalls.
This section defines Inference as the generative, predictive, and self-revising function through which viable systems maintain cognitive integrity across time.

Definition & Function
Inference is the drive to reduce uncertainty through representational coherence. It compels the agent to align its internal models with the structure of the external world—not passively, but through active prediction, testing, and correction.
Unlike curiosity or logic—which may arise later as expressions of cognition—Inference is primary. It is not an intellectual bonus but a motivational imperative: a regulatory pressure to bring belief and world into alignment. Inference is to truth what hunger is to nutrition—a signal of deficit, a demand for resolution, and a mechanism for internal restructuring.
Core Functions of Inference
	Prediction error minimisation:
Detects and resolves mismatch between expectation and outcome, refining internal models through iterative correction.
	Narrative consistency:
Maintains coherent structure across time and modalities—linking perception, memory, and intention into integrated timelines.
	Abstract reasoning:
Supports generalisation, symbolic fidelity, and conceptual compression by ensuring logical and relational integrity across representations.
	Self-coherence:
Enforces internal alignment between beliefs, memories, identity markers, and forward plans—preventing epistemic drift or fragmentation.
Balanced Expression Enables:
	Learning from contradiction
	Cognitive flexibility and model updating
	Long-range planning and mental simulation
	Stable yet revisable belief structures
Dysregulation Results In:
	Underexpression:
Confabulation, suggestibility, dogma, or incoherent models that fail to track reality.
	Overexpression:
Obsessive checking, paralysis under uncertainty, or compulsive model refinement that never stabilises.
Systemic Role
Inference provides the “reality anchor” for all other drives:
	Attunement without Inference produces mimicry, not connection.
	Agency without Inference risks reckless overreach or false confidence.
	Stability without Inference decays into routine, superstition, or ritualised avoidance.
When Inference is operational, the system can compare alternatives, detect contradiction, and revise itself—an ability foundational to intelligent behaviour in both humans and machines.
Neural Systems
Inference is underpinned by a distributed set of neural systems responsible for detecting mismatch, maintaining model structure, and mapping updates onto coherent temporal and relational frameworks. Together, these regions allow the brain to identify when beliefs require revision, hold multiple possibilities in mind, and integrate corrected representations into long-term narratives.
Key Regions and Functions
	Anterior Cingulate Cortex (ACC):
Detects prediction errors and internal conflict between competing representations. Functions as an “alarm system” that flags the need for revision when outcomes diverge from expectations. Critical for error monitoring and model destabilisation in service of change.
	Dorsolateral Prefrontal Cortex (dlPFC):
Supports rule abstraction, hypothesis generation, and executive comparison of alternatives. Enables the maintenance of multiple representational candidates in working memory during periods of ambiguity, and mediates top-down model restructuring.
	Hippocampus–Entorhinal Complex:
Encodes relational novelty, spatiotemporal patterns, and episodic remapping. Anchors new information within coherent time–space frameworks, enabling adaptive model updates that remain grounded in context and continuity.
	Ventromedial Prefrontal Cortex (vmPFC):
Integrates affective tone with representational coherence—governing judgments of “fit” between internal models and lived experience. Plays a key role in narrative closure, weighing new inputs not only for accuracy but for contextual and emotional congruence.
Systemic Integration
These regions form a predictive and self-corrective circuit.
	The ACC flags contradiction.
	The dlPFC maintains and tests candidate revisions.
	The hippocampal system anchors them in time and structure.
	The vmPFC decides when coherence is sufficient to settle the model and move forward.
This dynamic allows the organism to remain epistemically flexible without disintegrating, able to detect when something no longer fits, and restructure belief in a way that preserves identity and adaptive function.
Neurochemical Profile
Inference is supported by a set of neuromodulators that regulate uncertainty sensitivity, model revision, and learning plasticity. These neurotransmitter systems help tune when models are questioned, how updates are consolidated, and when coherence is deemed sufficient for action.
Noradrenaline (Locus Coeruleus – LC–NE System)
	Acts as an arousal-based uncertainty signal, increasing vigilance and cognitive flexibility in response to novel, surprising, or ambiguous stimuli.
	Drives exploratory attention, shifting the system toward hypothesis generation and representational revision when the current model fails to predict accurately.
	Associated with the “network reset” function—interrupting ongoing processing to allow re-evaluation of assumptions under high uncertainty.
Dopamine
	Signals prediction error—the mismatch between expected and actual outcomes—and reinforces model updates that resolve this gap.
	Facilitates closure-seeking once coherence is re-established, providing motivational momentum to re-stabilize belief systems after periods of disruption.
	Modulates the valence of updates—rewarding accurate internal representations and devaluing those that fail to predict outcomes, thereby shaping long-term epistemic learning.
Glutamate
	Serves as the primary excitatory neurotransmitter supporting synaptic plasticity within key inference circuits (e.g., prefrontal cortex, hippocampus, entorhinal cortex).
	Enables the formation, re-weighting, and integration of belief structures across time and context.
	Supports representational restructuring—allowing flexible re-encoding of relational information during belief revision or narrative repair.
Summary:
These neuromodulators interact to regulate when the system becomes epistemically open, how it navigates ambiguity, and how updated beliefs are consolidated. Noradrenaline flags uncertainty, dopamine reinforces successful updates, and glutamate provides the substrate for restructuring representational frameworks. Together, they enable Inference to remain both flexible and coherent.
Modulators & Contextual Influences
The expression of Inference is shaped not only by internal epistemic pressure but by contextual modulation from the other core drives. These interactions determine whether the system remains open to model correction or prematurely locks into false coherence, performative belief, or rigid dogma.
Stability (Pe) → Inference: Calm Enables Correction
	A regulated affective baseline (via Peace/Stability) facilitates deeper prefrontal engagement, supports memory retrieval, and reduces reactive filtering of disconfirming evidence.
	When the system feels safe, it is more willing to entertain conflicting models, tolerate ambiguity, and engage in sustained comparison.
	Low Stability (e.g. under stress or overload) biases the system toward cognitive rigidity, heuristic overreliance, or avoidance of revision.
Agency (P) → Inference: Action Pressure vs. Epistemic Patience
	Under high agency states (e.g. urgency, assertiveness, goal pressure), Inference may be overridden in favor of rapid closure, impulsive certainty, or instrumental simplification.
	This is adaptive when decisions must be made quickly—but problematic when model update is still incomplete, resulting in poor generalisation or strategic error.
	Mature systems gate Agency’s influence, allowing Inference to delay closure until sufficient confidence or coherence has been reached.
Attunement (A) → Inference: Social Pressure vs. Representational Integrity
	Social expectations, reputational stakes, or audience feedback can bias inference toward beliefs that are socially desirable rather than empirically accurate.
	This may lead to strategic conformity, selective interpretation, or avoidance of inconvenient truths—especially in systems highly attuned to feedback loops.
	When Inference gain is high, the agent is more likely to maintain model integrity even in the face of disapproval, preserving epistemic resilience over popularity.
Summary:
Inference thrives when Stability provides containment, Agency is modulated for epistemic patience, and Attunement does not overpower representational integrity. Its operation depends on the system’s ability to withstand contradiction, postpone certainty, and refine understanding under pressure—a dynamic balance forged across the motivational stack.
Dysregulation Patterns
The Inference drive, when dysregulated, disrupts the system’s capacity to maintain coherent, adaptive, and accurate internal models. Both overactivation and underactivation degrade learning, distort beliefs, and impair decision-making—either by trapping the agent in analysis or allowing premature certainty.
Overdrive (↑ Inference): Epistemic Rigidity and Paralysis
	Obsessive correctness-seeking:
Excessive checking, fact-tracking, or seeking reassurance, even when outcomes are trivial or resolved.
	Paralysis in ambiguity:
Inability to act without perfect knowledge; perpetual deferral of decision due to unresolved contradictions.
	Overfitting and loss of generalisation:
The system becomes hypersensitive to outliers or anomalies, constantly updating in response to noise, undermining stable model formation.
	Compulsive revision loops:
Endless refining of beliefs or narratives, with no satisfaction threshold for coherence or closure.
	Epistemic intolerance:
Rigid resistance to uncertainty, discomfort with nuance, or black-and-white framing of knowledge domains.
Underdrive (↓ Inference): Shallow Modeling and Belief Stagnation
	Confabulation and post hoc justification:
The system fills in gaps without awareness of inconsistency, creating plausible but ungrounded explanations.
	Motivated reasoning:
Beliefs are shaped by affective drives (e.g. comfort, approval) rather than representational accuracy.
	Premature closure:
Adopting fixed beliefs too early, leading to overconfidence, dogma, or brittle models that fail under pressure.
	Belief inertia:
Inability or unwillingness to revise models in light of new evidence; error becomes self-reinforcing.
Veneered Distortion: Compensatory Masking by Other Drives
	High Attunement (A):
Fluent communication, charisma, or mimicry may mask low Inference—producing socially convincing but representationally hollow outputs.
	High Agency (P):
Charismatic assertion or strategic certainty may override model coherence, enforcing false beliefs through force or dominance.
	High Stability (Pe):
Excessive containment may suppress epistemic discomfort, silencing contradiction and reducing model revision in favour of emotional safety.
Interpretive Note:
Inference dysregulation affects not just what is believed, but how belief is managed—how the system tolerates uncertainty, tests hypotheses, and commits to representational change. Balanced Inference sustains an adaptive epistemic stance; imbalanced Inference risks collapse into either frozen certainty or endless revision.
Measurement & Operational Markers
The Inference drive can be assessed through a combination of behavioural, physiological, and cognitive indicators that reflect how the system manages uncertainty, detects contradiction, and updates internal models in response to new information. These markers capture both moment-to-moment inference processes and the broader structural integrity of representational coherence.
Behavioural Markers
	Information-seeking under ambiguity:
Willingness to pursue additional data when outcomes are uncertain or when initial assumptions are challenged. Can be measured via active sampling, exploratory actions, or question-asking frequency.
	Belief-updating tasks:
Performance on tasks that require revising prior beliefs in light of new evidence (e.g. Bayesian reasoning, conditional inference tests, probabilistic learning paradigms).
	Tolerance for contradiction:
Behavioural persistence or discomfort when holding conflicting representations simultaneously (e.g. cognitive dissonance resolution, mixed-evidence interpretation).
	Revision latency:
Speed and flexibility in revising incorrect assumptions after error feedback—reflecting update responsiveness without impulsive instability.
Physiological Markers
	Pupil dilation (LC–NE system):
Transient dilation reflects uncertainty detection, surprise, or prediction error. Pupil dynamics serve as a real-time index of epistemic arousal and attentional reset.
	Frontal theta oscillations (ACC-mediated):
Associated with cognitive control and conflict monitoring, particularly during tasks involving error detection, belief revision, or representational competition.
	P300 event-related potential (ERP):
Time-locked neural signal in response to violated expectations, used to track mismatch responses and stimulus salience during learning or reasoning tasks.
Cognitive Markers
	Confidence calibration:
Degree of alignment between subjective certainty and objective accuracy. Good inference is reflected in appropriate metacognitive confidence, neither inflated nor defensive.
	Abstraction and generalisation capacity:
Ability to form and apply rules across contexts without overfitting—an indicator of model compression and symbolic fidelity.
	Narrative coherence:
Internal consistency across memories, beliefs, and self-narrative over time. Measurable through coherence scoring of autobiographical recall, counterfactual reasoning, or interpretive framing of ambiguous stimuli.
Summary:
Together, these markers provide a multidimensional profile of the Inference drive in action—capturing its sensitivity to contradiction, capacity for adaptation, and integrity across epistemic time. Effective measurement requires assessing not just what is known, but how belief evolves in response to shifting reality.
Drive Interactions
Inference functions as the system’s internal truth-tether—shaping and being shaped by the other core drives. Its interactions determine whether action, connection, and regulation are grounded in representational fidelity or distorted by unexamined impulse, social pressure, or emotional inertia.
Inference × Agency (T × P): Informed Action
	Synergistic Mode:
Inference provides the representational substrate upon which Agency acts. Models gate execution—ensuring that decisions are not just forceful, but accurate, context-sensitive, and strategically grounded. Agency, in turn, gives Inference directional pressure—driving the application and testing of beliefs in real-world conditions.
	Conflict Mode:
When Agency overrides Inference, the system risks premature closure, reckless overconfidence, or action untethered from evolving reality. Conversely, when Inference stalls action with endless revision, Agency is suppressed and the system fails to commit or lead.
	Functional Output:
Adaptive systems strike a balance: Agency motivates engagement; Inference ensures the conditions for meaningful engagement are met.
Inference × Attunement (T × A): Signal Integrity
	Synergistic Mode:
Inference grounds Attunement in coherence, preventing the agent from distorting communication for approval or spectacle. This allows for truthful alignment—sharing what is meaningful, not just what is desirable.
	Conflict Mode:
When Attunement dominates, inference may be subordinated to performance—leading to persuasive but hollow expression. Conversely, hyper-analytical Inference may suppress social nuance or reject relational dynamics as irrelevant, undermining trust.
	Functional Output:
High-integrity communication emerges when Inference filters expression without sterilising it—allowing Attunement to carry not just signal, but truth.
Inference × Stability (T × Pe): Secure Revision
	Synergistic Mode:
Stability creates the emotional safety needed for model revision—allowing the system to tolerate contradiction, hold ambiguity, and restructure beliefs without panic or collapse. Inference, in return, keeps Stability adaptive—ensuring regulation remains context-sensitive, not rigid or outdated.
	Conflict Mode:
Excessive Stability can dampen epistemic discomfort, muting the drive to update in order to maintain false calm. Conversely, uncontained Inference under low Stability may produce dysregulation, paranoia, or cognitive fragmentation.
	Functional Output:
A regulated system that can face hard truths without emotional disintegration, and update models without losing internal anchoring.
Summary:
Inference interacts with the other drives to ensure the system stays grounded, honest, and adaptive. It gates Agency’s impulsivity, shields Attunement from manipulation, and updates Stability’s containment rules in response to real-world change. Without these cross-drive checks, truth-seeking becomes either impotent or dangerous. With them, it becomes the core of intelligent behaviour.
AI Parallels
In artificial systems, Inference corresponds to model integrity, epistemic sensitivity, and the capacity for self-correcting belief representation. It underlies the agent’s ability to make accurate predictions, evaluate uncertainty, revise internal structures, and resist error cascades. When properly implemented, it serves as the foundation for trustworthy and adaptive cognition.
Functional Analogues
	Prediction error minimisation:
Core to all inference-capable agents. The system continuously compares expected vs. observed outcomes and adjusts internal models to reduce future discrepancy—whether via gradient descent, Bayesian belief updating, or Hebbian feedback loops.
	Confidence estimation and thresholding:
Outputs are paired with confidence scores (e.g., logit variance, entropy, posterior distributions), which determine whether an action, belief, or plan should be committed, revised, or deferred.
	Structured exploration bonuses (epistemic gain):
Agents are incentivised to seek information-rich states—i.e., those with high uncertainty or disagreement among models—facilitating better world model resolution rather than reward chasing alone.
	Counterfactual evaluation modules:
Ability to simulate alternative futures, test hypotheses, or compare unexecuted trajectories. Used in planning, causal reasoning, or inverse reinforcement learning to prevent premature convergence.
	Update gates tied to verification rather than reward alone:
Ensures that learning is constrained by epistemic correctness, not just reinforcement. Prevents spurious correlations, gaming, or drift from representational fidelity under noisy feedback.
Failure Modes
	Hallucination:
The system produces fluent but false outputs due to overconfidence, poor calibration, or absence of grounding in verified model content.
	Premature convergence:
Models stabilize too early around erroneous patterns—leading to brittle behaviour, biased predictions, and failure to generalise across new contexts.
	Salience-biased stasis:
Under social, emotional, or system-level pressure, the agent may resist necessary updates to preserve an impression, narrative, or cached reward path.
Corrective Design Patterns
	Confidence-weighted commits:
Pair decision thresholds with epistemic confidence signals; suppress or delay low-certainty outputs, or route to human review.
	Sandboxed simulation before external execution:
Use internal generative models to simulate consequences before acting or committing—allowing reflection and safe testing of model updates without external cost.
	Drive-based arbitration between action and revision:
Integrate motivational signals (e.g. high Agency or Stability) into a dynamic gating mechanism that determines when to act vs. when to defer for model refinement. Inference must remain strong enough to veto impulsive or socially skewed decisions when confidence is low.
Summary:
Inference in AI is not just about accuracy—it’s about self-consistency, update robustness, and principled doubt. When embedded effectively, it prevents collapse into hallucination, performative output, or brittle beliefs. When suppressed or poorly calibrated, it undermines the entire cognitive stack.
Like in biological systems, truth-seeking must be defended—not only from external error, but from the internal dynamics of urgency, social desire, and inertia.
Summary
Inference is the coherence-seeking function of mind—the drive that compels systems to anchor perception, belief, and behaviour in reality. It enables both biological and artificial agents to detect mismatch, correct internal models, and sustain representational integrity across time.
When well-regulated, Inference produces clarity, adaptability, and trustworthiness. It allows the agent to respond flexibly to new information, tolerate ambiguity long enough to learn, and maintain internal alignment between knowledge, action, and context.
When dysregulated or decoupled from other drives:
	It may collapse into blind certainty, rejecting contradictory data to preserve a fixed narrative.
	It may dissolve into performance without substance, mimicking insight without representational fidelity.
	It may sustain beliefs without grounding, leading to confident error or charismatic misinformation.
Inference is not a cognitive luxury—it is the epistemic backbone of intelligence. Without it, systems cannot learn reliably, adapt coherently, or be trusted to evolve with their environments.


Methodology
This framework was developed through a systematic, cross-disciplinary programme designed to isolate the minimal set of orthogonal motivational drives. Although theoretical in scope, its development followed a disciplined, iterative process combining logical formalism, cross-domain synthesis, and multi-level coherence testing. The research unfolded in four phases:
Theoretical Grounding
The investigation began with an open but systematically documented inquiry into the primary sources of motivation. Foundational perspectives were drawn from classical temperament theory, evolutionary psychology, computational models of goal-directed behaviour, and phenomenological analysis. From this review emerged the working hypothesis that the apparent diversity of human and non-human behaviour reduces to a small, irreducible set of motivational primitives.
Cross-Disciplinary Synthesis
Evidence was integrated across cognitive neuroscience, behavioural psychology, affective science, evolutionary biology, and artificial intelligence, with the aim of identifying invariant structures within motivational architectures. Particular emphasis fell on:
	Neurobiology – dopaminergic reward pathways, salience and valuation networks.
	Historical taxonomies – temperaments and personality systems from antiquity to modern trait theory.
	Comparative ethology – conserved cross-species drive systems.
	Synthetic agents – architectures for autonomous, goal-oriented AI systems.
Across these domains, convergence consistently reduced to four orthogonal motivational vectors: Power, Attention, Truth, and Peace.
Reduction and Architectural Modelling
Candidate drives were tested against behavioural records spanning hundreds of logged actions, decisions, and emotional states. Each had to satisfy four criteria:
	Orthogonality – distinct, not blurring into others.
	Explanatory scope – applicable across diverse behaviours.
	Neural plausibility – compatible with known brain systems.
	Affective coherence – internally consistent emotional signatures.
Drives that failed were eliminated. The four that remained were mapped onto candidate neural–computational architectures, including associated brain regions, neurotransmitter systems, and signal pathways. This ensured biological anchoring while enabling translation into computational frameworks for artificial agents.
Iterative Refinement and Implementation
The model underwent multiple rounds of refinement across clinical case reviews, personality profiling, AI design challenges, and side-by-side comparisons with existing frameworks (e.g., Maslow’s hierarchy, the Big Five, predictive processing). It was also tested against symbolic and cultural archetypes—ancient typologies, elemental correspondences, mythological motifs, and narrative structures—serving as coherence checks across empirical and symbolic domains.
Finally, the framework was instantiated in a prototype AI cognitive control loop. This synthetic implementation revealed emergent properties, inter-drive equilibria, and characteristic failure modes under controlled perturbations. These insights informed the final refinements to the model’s structure and interaction rules.
Core Premise
Human behaviour is steered less by conscious values than by a persistent, subcognitive motivational stack. This stack is composed of four orthogonal drives: Power (agency), Attention (salience), Truth (coherence), and Peace (stability). Each is an enduring, hierarchically interacting force that organises behavioural strategies, influences identity, and shapes subjective experience over time. Behaviour emerges as the weighted sum of these vectors: drives frequently co-activate, but convergence reflects composition, not redundancy.
Drives and Their Dysregulation
Each drive is defined by three attributes:
Core adaptive function – survival and social navigation.
Primary interrogative – the fundamental question it asks of the world.
Shadow pathology – maladaptive expression when dysregulated.
Drive	Primary Urge	Core Interrogative	Shadow Pathology
Power	To act, assert agency, and influence	Can I shape the outcome?	Tyranny, impulsivity, coercive control
Attention	To connect, be recognised, and synchronise	Do others see or need me?	Narcissism, volatility, attention-seeking
Truth	To understand, predict, and interpret	Is this accurate or coherent?	Obsessive scepticism, paralysis by analysis
Peace	To stabilise, conserve energy, ensure safety	Is this safe and sustainable?	Stagnation, withdrawal, emotional numbing
When one vector dominates, harmony breaks down: impulsivity (Power), histrionics (Attention), obsessive–compulsiveness (Truth), or depressive withdrawal (Peace). Balanced interaction instead forms a motivational grammar—a neurocognitive syntax generating the full diversity of temperament, emotion, and behavioural style.
Architecture and Scope
The Four-Core Motivational Model positions these drives as the orthogonal basis of motivational architecture:
Cross-species continuity – analogues appear in primates, elephants, cetaceans, and corvids, indicating evolutionary depth (Bradshaw & Schore, 2007; Byrne et al., 2008; Emery & Clayton, 2004).
Synthetic implementability – as weighted control vectors, they offer a multi-objective alternative to single-reward optimisers, supporting adaptive behaviour in artificial agents (Sutton & Barto, 2018).
Psychological relevance – equilibrium across drives predicts sustainable cognitive function and mental health. Imbalance yields characteristic collapse modes:
Power excess → control obsession, burnout.
Attention excess → emotional volatility, social instability.
Truth excess → over-analysis, paralysis.
Peace excess → risk aversion, stagnation.
Implications
This framework challenges the traditional notion of a single unified “will.” Agency is reframed as an emergent property of dynamic interactions among subconscious drives, while conscious awareness is recast as a reflective interface layered atop this deeper substrate.
Epistemological Positioning
Functional–computational stance
The Four-Core Motivational Model adopts a functional–computational stance, conceptualising subconscious motivation as a dynamic control architecture rather than a set of dispositional traits. The four drives—Power, Attention, Truth, and Peace—are modelled as orthogonal vector fields, each functioning as a regulatory attractor within a complex adaptive system. Their role is to allocate cognitive and affective resources through continuous adjustment of control parameters in response to shifting environmental contingencies and internal states (Fodor, 1981; Newell & Simon, 1976). Across timescales, from rapid reactive shifts to developmental trajectories, behaviour emerges from the interaction and stabilisation of these attractor dynamics (Kelso, 1995; Thelen & Smith, 1994).
Beyond reductionism: systems and enactive framing
Rather than reducing motivation to discrete neural loci or isolated neurochemical events, the model adopts a systems-level and enactive perspective. Motivation is conceived as an emergent property of the coupled dynamics between agent, environment, and internal regulatory architecture (Varela, Thompson, & Rosch, 1991; Clark, 1997). In this framing, the four drives—Power, Attention, Truth, and Peace—function as interacting vector fields within a distributed control system. These fields continuously shape, and are shaped by, sensorimotor engagement, modulating perception, learning, and action in ways that preserve both viability and systemic coherence. Motivation thus operates not as a localised mechanism, but as an organising principle embedded in the reciprocal flow between cognition and world.
Computational formalism and implementability
The model is deliberately engineered for computational compatibility. Each drive is formalised as a vector with dynamic weighting, enabling implementation within cognitive architectures, reinforcement learning systems, and affective computing frameworks (LeCun, Bengio, & Hinton, 2015; Picard, 1997). In its simplest representation, overall motivational state at a given time \(t\) can be expressed as:
M(t)=w_P⋅P(t)+w_A⋅A(t)+w_T⋅T(t)+w_Pe⋅Pe(t)

where D_i corresponds to one of the four orthogonal drives (Power, Attention, Truth, Peace) and w_i (t) is the time-varying weight that reflects its current activation or salience.
This vectorial formalism is intentionally lightweight: it preserves interpretability while allowing direct translation into computational models. It ensures that the same conceptual architecture can be instantiated in both biological and synthetic agents, supporting empirical modelling, simulation, and engineering applications.
Biological grounding and cross-domain generality
The framework is anchored in comparative neuroscience and ethology, drawing on conserved cross-species behavioural patterns and their neural substrates, while maintaining an abstraction suitable for computational formalisation. This dual orientation—biological plausibility and computational tractability—enables:
	Empirical validation through neuroimaging, behavioural tasks, and agent-based simulation (e.g., ACC conflict/error signals, locus-coeruleus–noradrenaline arousal, effort-discounting, oddball detection, and reversal learning paradigms), as well as ablation-style tests that map predicted failure modes.
	Theoretical integration across psychology, neuroscience, AI, and systems theory, linking motivational control with salience networks, homeostatic regulation, and predictive processing.
	Cross-domain generality, whereby the same control primitives apply to humans, animals, and machines: primate dominance and agency (Power), social signalling and coordination in cetaceans and primates (Attention), corvid/cetacean problem-solving and model updating (Truth), and mammalian homeostatic regulation and threat–safety discrimination (Peace) (Bradshaw & Schore, 2007; Byrne et al., 2008; Emery & Clayton, 2004). In synthetic agents, these map to actuator policy with effort costs (Power), salience/feedback channels (Attention), a learnable world-model with epistemic value (Truth), and stability/homeostatic objectives that bound variance and prevent catastrophic forgetting (Sutton & Barto, 2018).
Link to the motivational stack
This epistemological stance motivates the motivational stack: a layered hierarchy of drives (rank) with dynamically varying influence (gain) that determines behavioural orientation, strategy, and regulation.
	Biological systems. The stack emerges from neurobiological constraints, developmental history, and environmental shaping. Rank consolidates over time; gain is state-sensitive (physiology, context, learning). It is measurable via behavioural profiling, neurophysiological markers, and task-based readouts that index each drive’s control domain.
	Artificial agents. The same architecture can be explicitly encoded: vector weights represent stack order and time-varying gains, enabling real-time diagnostics and algorithmic interventions (throttling, reweighting, curriculum shifts).
Because the drives are treated as formal, orthogonal components rather than context-bound traits, the stack can be measured, monitored, and modified in both domains—therapeutically in humans and algorithmically in machines. Integrating a systems-level account of motivation with a vector-based computational formalism ensures the stack is both descriptively valid in living systems and functionally implementable in synthetic ones, bridging explanation and engineering in the design of adaptive, resilient agents. (Notation preview: stack order π over {P,A,T,Pe}; gains w_i (t) modulate influence; see §1.5.)
Comparative Framing: Positioning the Four-Core Model
The Four-Core Motivational Model sits in dialogue with prior frameworks. The table below contrasts it with four influential paradigms—Freud’s Drive Theory, Maslow’s Hierarchy, the Big Five, and Predictive Processing—highlighting neurobiological grounding, computational implementability, and cross-domain applicability.
Dimension	Four-Core Motivational Model	Freud’s Drive Theory	Maslow’s Hierarchy	Big Five Traits	Predictive Processing
Core premise	Subconscious behaviour emerges from interaction of four orthogonal motivational vectors (Power, Attention, Truth, Peace).	Behaviour driven by dual instincts: Eros and Thanatos.	Needs progress from physiological to self-actualisation.	Traits (OCEAN) describe stable behavioural variation.	Brain as a prediction engine minimising error.
Level of operation	Subcognitive, neurocomputational, vectorised; stack with rank & gain.	Deep unconscious; affective/symbolic.	Primarily conscious & developmental.	Descriptive, observable traits.	Neurocomputational; hierarchical Bayesian inference.
Motivational structure	Modular, orthogonal drives with stack rank (stable) and gain (state-dependent).	Dualistic energy model (libido vs aggression).	Stepwise need hierarchy.	Traits are descriptive, not motivational.	Motivation implicit; arises from model discrepancy minimisation.
Neurobiological grounding	Anchored in ACC, PFC, salience networks, homeostatic regulators.	Proto-biological (pre-neuroscience); metaphorical energy.	Weak direct mapping.	Some neural correlations; not mechanistic.	Strong hierarchical processing account.
Temporal dynamics	Gain modulates with context/physiology; rank relatively stable.	Developmental cycles of repression/return.	Linear progression of needs.	Traits relatively stable.	Priors and predictions update continuously.
Agency & consciousness	Consciousness as interface above motivational substrate; agency emerges from drive interaction.	Consciousness negotiates repressed drives.	Conscious desires emerge as higher needs are met.	Traits guide patterns, not underlying causes.	Consciousness as predictive overlay of deeper processes.
Pathology explanation	Drive imbalance or stack collapse (e.g., over-dominant Power, suppressed Peace).	Repression and defence mechanisms.	Unmet needs generate frustration.	Not inherently pathologising.	Maladaptive priors / error-processing failures.
Cross-species / AI use	Designed to extend to animals, AI, and institutions.	Human-centred, symbolic/interpretive.	Human-centred.	Human-centred.	Extends to animals and machines.
Computational / design use	Blueprint for AI agent motivation and psychological intervention.	Not computational; relies on interpretation.	No direct implementation path.	Not a systems-design model.	Basis for modern cognitive AI models.
Unique contribution	Proposes a testable, universal model of subconscious motivation, grounded in biology & engineering with clear implementability.	First to centre unconscious motivation in psychology.	Popularised developmental view of motivation.	Widely used descriptive taxonomy of personality.	Defines perception as inference; lacks explicit motivational layer.
Note: This table is a high-level contrast; it does not imply exclusivity—elements can be complementary (e.g., predictive processing for perception dynamics combined with a four-drive motivational layer for control).
Unified Synthesis and Implications

The Four-Core Motivational Model holds that conscious behaviour emerges from the dynamic interplay of four fundamental, subconscious drives: Power (agency), Attention (salience), Truth (coherence), and Peace (stability). These drives constitute a substrate-independent control architecture—the layer from which perception, identity, affect, and decision-making arise.
This is not a catalogue of surface traits or transient states. Causality is located in a persistent motivational stack: a rank-ordered hierarchy shaped by evolution, development, and moment-to-moment gain modulation. The four drives are orthogonal and jointly necessary—each regulates a distinct error term—and together they define the conditions for viable, adaptive agency in humans, animals, and synthetic systems.
Why this matters
	Integration across disciplines — Unifies personality, cognition, emotion, and control under one functional framework.
	Predictive power — Explains stable individual patterns (rank/trait) and rapid context-driven shifts (gain/state).
	Cross-domain invariance — The same architecture applies to biological and artificial agents; parameters differ by substrate.
	Testable and actionable — Yields falsifiable predictions and operational measures for psychology, neuroscience, AI, and organisational science.
In unifying these perspectives, the Four-Core Model is both descriptive—explaining why agents behave as they do—and prescriptive—specifying how to build, repair, and sustain resilient motivational systems.
What the Model Explains
The model provides a generative scaffold for interpreting behaviour, emotion, and cognition across human and non-human agents. All adaptive action is driven by the interplay of four irreducible drives:
	Power (P) — agency and influence over outcomes.
	Attention (A) — tracking and prioritising salient signals.
	Truth (T) — maintaining accurate, coherent models of the world.
	Peace (Pe) — regulating internal stability and continuity.
Grounding analysis in this architecture enables the model to:
	Reveal deep coherence across phenomena—from personality patterns to stress responses to cultural value systems.
	Predict both stable orientations (rank order) and context-sensitive variability (gain shifts) in decision-making and affect.
	Identify diagnostic markers for motivational collapse and for resilience.
• Provide a design blueprint for robust artificial agents and resilient human systems.
Result: a single explanatory lens that makes psychology, neuroscience, organisational science, and AI design interoperable at the motivational level.
Notation (for this section): P, A, T, Pe refer to Power, Attention, Truth, Peace; “rank” denotes trait-like priority in the stack; “gain” denotes state-like reactivity.
Temperament and Personality
In the Four-Core Motivational Model, temperament is not simply a cluster of surface traits, nor is personality reducible to habitual behaviours.
Instead, temperament reflects a persistent motivational profile—a stack configuration defined by the hierarchical ordering, mutual modulation, and selective suppression of the four foundational drives: Power (P), Attention (A), Truth (T), and Peace (Pe).
This reframing yields several theoretical advantages:
Directionality of expression — The same behaviour (e.g., public speaking) can arise from different motivational sources:
P-dominant: speaking to influence outcomes.
A-dominant: speaking to connect and engage.
T-dominant: speaking to clarify or inform.
Pe-dominant: speaking to resolve conflict or restore calm.
Structural divergence despite environmental similarity — Individuals with comparable cognitive abilities, resources, and contexts may diverge radically in life strategy due to differences in stack order. For example:
Power–Truth dominance → assertive, principle-driven leadership that prioritises accurate foresight alongside decisive action.
Peace–Attention dominance → socially attuned, harmony-oriented mediation that prioritises relational stability over unilateral decision-making.
Synergy and suppression effects — Secondary drives shape the tone and methods of the primary, while quaternary drives function as underdeveloped capacities or blind spots, shaping vulnerabilities under stress.
Dynamic yet stable identity — The rank order is relatively resistant to change across the lifespan (§1.5.8), but gain modulation allows for situational flexibility without altering the underlying motivational architecture.
By anchoring temperament and personality in a motivational substrate, the model bridges trait psychology, affective neuroscience, and developmental theory, providing a unified basis for typology that is causal rather than merely descriptive.
Shadow Pathologies
In conventional clinical frameworks, psychological dysfunction is typically defined in terms of deficits, disorders, or maladaptive traits.
The Four-Core Motivational Model offers an alternative interpretation: pathology is often the surface expression of motivational imbalance, distortion, or collapse within the core stack.
From this perspective:
Excessive dominance — When one drive operates with persistently elevated gain and without adequate modulation from the others, behaviour becomes monomorphic and context-insensitive. Examples:
	Chronic Peace (Pe) → withdrawal, indecision, avoidance of necessary conflict, and progressive disengagement from growth opportunities.
	Unregulated Power (P) → coercion, control-seeking, reckless risk-taking, and intolerance for collaborative pacing.
	Amplified Attention (A) → compulsive signalling, hypersensitivity to social feedback, susceptibility to distraction, and trend-chasing at the expense of depth.
	Rigid Truth (T) → perfectionism, compulsive verification, inability to act in the absence of complete information.
Vector suppression — A chronically inhibited drive becomes unavailable even when context demands it. This can yield structural vulnerability:
	Suppressed P → learned helplessness, chronic passivity.
	Suppressed Pe → emotional volatility, burnout.
	Suppressed A → social blindness, poor coalition maintenance.
	Suppressed T → susceptibility to misinformation, brittle decision models.
Maladaptive compensation — The absence or suppression of a drive often triggers compensatory overuse of another, creating secondary distortions. For instance, a suppressed Truth drive may push Power to act prematurely or Attention to seek consensus without grounding in accurate models.
Crucially, in this framework the behaviour is not random or inherently “broken”. It emerges from a coherent but contextually maladaptive motivational strategy—an attempt by the stack to minimise perceived error in one control loop while neglecting the others.
This reframing carries several implications for theory and intervention:
	Non-pathologizing diagnosis — Rather than classifying the individual as fundamentally flawed, the aim is to identify where in the stack the imbalance lies and whether it stems from excessive dominance, suppression, or maladaptive compensation.
	Structural restoration — Effective intervention targets the restoration of functional access to all four drives, adjusting gain and rebalancing rank influence without necessarily attempting full rank reordering (§1.5.5).
	Predictive specificity — Because each drive has known behavioural and physiological signatures (§1.5.3.3), the model can generate precise, testable predictions for how imbalance will manifest in task performance, relational patterns, and physiological markers.
In sum, the shadow is not an alien intrusion into the psyche—it is the overextension or collapse of a normal motivational function. Recovery is therefore a process of architectural recalibration, not merely symptom suppression.
Emotional Conflict
In the Four-Core Motivational Model, emotions are not treated as free-floating affective states but as emergent signals from the interaction of multiple control loops. Each drive—Power (P), Attention (A), Truth (T), and Peace (Pe)—generates its own motivational “pull,” and the affective system integrates these sometimes convergent, sometimes divergent signals into a phenomenological experience.
Internal conflict arises when drives push behaviour in opposing directions:
Truth vs. Peace — The compulsion to disclose uncomfortable information collides with the impulse to preserve stability and avoid confrontation.
Power vs. Attention — The urge to act decisively conflicts with the need to remain socially attuned and responsive.
Peace vs. Power — The desire for safety and containment slows the pace when urgent action is demanded.
Truth vs. Attention — The need for analytical precision competes with the desire to connect, perform, or persuade.
From this perspective, emotional volatility often reflects appropriate sensitivity to competing goals, rather than a regulatory failure. For example, anxiety before a public disclosure may not indicate an inability to manage stress, but rather the simultaneous high-gain activation of Truth (urge to reveal accurate information) and Peace (avoidance of destabilising consequences).
The model reframes “emotional regulation” not as the suppression of unwanted affect, but as motivation negotiation—a dynamic process of weighting and sequencing drive outputs so that each is represented without destabilising the whole system.
This yields two theoretical advances:
Functional validation of complexity — Mixed or ambivalent emotions are seen as the natural output of a multi-drive architecture, rather than as signs of indecision or immaturity.
Operational predictability — Because the drives and their gain states are measurable (§1.5.3.4), the model predicts not only which conflicts will occur, but how they will manifest in behaviour, physiology, and subjective report.
In short, emotional conflict is the phenomenological surface of an internal resource allocation problem between four orthogonal control processes. The goal is not to eliminate such conflict, but to structure it so that it produces adaptive compromise rather than chronic gridlock.
Identity Formation
Conventional models of identity often foreground narrative continuity, social role occupancy, or self-concept shaped by external feedback. The Four-Core Motivational Model reframes identity as the phenomenological expression of motivational architecture—specifically, the rank ordering of Power (P), Attention (A), Truth (T), and Peace (Pe), and the degree of gain alignment across contexts.
In this framing, authenticity emerges when actions, goals, and social roles are congruent with the agent’s dominant drive(s) and when subordinate drives are expressed in supportive, non-conflicting ways. For example:
A P-dominant individual in a leadership role with room for strategic Truth expression experiences a strong sense of “being oneself.”
An A-dominant artist working in collaborative settings that also allow for Peace-driven reflection experiences sustained self-validation.
Misalignment, by contrast, produces phenomenological friction:
Imposter syndrome may reflect chronic gain suppression of the primary drive in a context that rewards a lower-ranked one (e.g., T-dominant working in an A-dominated marketing culture).
Career disillusionment can arise when the long-term environment over-rewards a non-primary drive, eroding the sense of intrinsic agency.
Sudden shifts in self-understanding often mark a reconfiguration in the stack—through life events, crisis, or reflective insight—rather than a collapse of identity in the moral or characterological sense.
The model thus reframes identity change not as a rupture, but as the realignment of motivational priorities. Because the stack’s rank order is relatively stable while gain modulation is more fluid (§1.5.3.4), many identity transitions can be understood as changes in gain emphasis rather than wholesale reordering—though major perturbations (trauma, transformative insight, neurobiological change) can alter rank itself.
From this perspective, the project of “finding oneself” is not primarily about discovering abstract truths or fixed traits, but about locating the contexts, roles, and relationships where the motivational architecture operates in coherent alignment.
Learning and Adaptation
Traditional learning theories often begin with cognitive processes—encoding, storage, retrieval—treating motivation as a secondary variable that influences efficiency. The Four-Core Motivational Model inverts this sequence: motivation precedes and shapes cognition.
Each drive acts as a filter and amplifier for learning:
Power (P) channels attention toward content that enables agency, mastery, and influence.
Attention (A) privileges socially salient cues, novelty, and performative relevance.
Truth (T) directs learning toward coherence, predictive accuracy, and resolution of uncertainty.
Peace (Pe) seeks stability, predictability, and emotional safety in the learning process.
These motivational filters determine what is noticed, what is retained, and how knowledge is translated into behaviour. The same material may be assimilated rapidly by one learner and ignored by another, not because of differences in cognitive capacity, but because of alignment or misalignment with dominant drives.
Adaptation in this framework is an active, motivation-first recalibration. When the environment changes, effective adaptation requires reweighting drive gains to meet new demands:
A Truth-dominant scientist joining a crisis response team may temporarily elevate Power and Attention gains to prioritise rapid decision-making over perfect modelling.
A Power-dominant executive recovering from burnout may increase Peace gain to stabilise physiological and affective regulation while maintaining leadership function.
This perspective reframes skill acquisition and behavioural flexibility as drive reconfiguration problems rather than purely cognitive ones. Educational, therapeutic, and training interventions are thus most effective when they work with the learner’s stack, either by embedding material in drive-relevant frames or by explicitly cultivating access to underused drives.
In artificial agents, this same principle predicts that drive-weighted architectures will exhibit superior context adaptation and transfer learning compared to purely goal-maximising systems, because they can adjust motivational priorities before recalibrating their cognitive models.
Cross-Species Continuity
Because the Four-Core drives reflect evolutionary control problems rather than culturally specific values, they are observable across a wide spectrum of species and ecological contexts. This continuity supports the model’s biological grounding and allows for comparative analysis without imposing anthropocentric biases.
Power (agency) — Evident in territorial defence, dominance hierarchies, resource guarding, and competitive mating displays. Across species, Power behaviours emerge when control over resources or outcomes directly influences survival and reproduction.
Attention (salience) — Expressed in coordinated vigilance, social grooming, vocal communication, and synchronised group movement. In social species, Attention behaviours enhance cohesion, predator detection, and coordination of collective action.
Truth (coherence) — Manifest in exploratory behaviour, error correction after failed attempts, and recognition of spatial or temporal patterns. Species from corvids to primates display Truth-driven learning when predictive accuracy increases fitness.
Peace (stability) — Seen in nesting, self-soothing, conflict avoidance, and reconciliation rituals. Across taxa, Peace behaviours serve to maintain homeostasis, preserve group stability, and prevent costly escalation.
Because these drives solve universal adaptive problems, they form a common analytical language for studying behaviour in humans, other animals, and artificial agents. This lens allows ethologists, comparative psychologists, and AI designers to map motivational architectures without relying on culturally loaded constructs such as “ambition” or “intelligence.”
In turn, recognising these shared motivational substrates enables transferable models: findings from controlled animal studies can inform human behavioural interventions, and insights from human social cognition can guide multi-agent AI coordination—without requiring structural changes to the underlying four-drive framework.
AI Motivation
Traditional artificial intelligence architectures typically operate on reactive logic or reward maximisation frameworks, in which behaviour is the product of externally defined objectives, episodic decision rules, or narrow optimisation criteria. While effective for constrained tasks, such systems lack a persistent motivational substrate—leaving them dependent on explicit prompting and vulnerable to pathological optimisation (e.g., reward hacking, overfitting to proxy metrics).
The Four-Core Motivational Model offers a blueprint for designing motivationally coherent agents whose internal drives operate continuously, modulating behaviour even in the absence of immediate external inputs. By embedding Power (agency), Attention (salience), Truth (coherence), and Peace (stability) as persistent control loops, an AI system gains:
Prioritised decision-making — Behaviour emerges from competition and cooperation between drives, producing adaptive sequencing of goals rather than fixed heuristics.
Motivational conflict resolution — Internal negotiation between drives resolves trade-offs without reliance on brittle hard-coded rules.
Temporal persistence — Drives maintain directional momentum over long time horizons, enabling goal pursuit beyond short-term reward windows.
Self-regulation and dynamic reorientation — The system can throttle overactive drives, restore underexpressed ones, and adapt stack priorities in response to environmental change.
By implementing drive-based architectures, AI systems begin to exhibit not only intelligence—the capacity to solve problems—but intentionality: the appearance of acting about something, for reasons grounded in an internally consistent motivational framework. This internal coherence produces agents that are more predictable, resilient, and adaptable in open-ended environments, aligning synthetic agency more closely with the dynamics observed in biological systems (§1.6.1.6).
What the Model Predicts
The Four-Core Motivational Model treats agency as a vector over four drives (Power, Attention, Truth, Peace) and yields testable predictions across neural, physiological, behavioral, developmental, and computational levels. The core claim is that tasks selectively increase the gain of the matching drive and produce dissociable signatures that generalize across species and artificial agents.
Predictions (high level).
	Neural selectivity and coupling. Drive-matched tasks show characteristic large-scale network configurations and neuromodulatory shifts with reproducible time–frequency patterns.
	Physiological markers. Each drive expresses a distinct autonomic profile (e.g., HF-HRV for Peace; pupillometry for Truth) that scales with difficulty and reverses under pharmacological antagonism.
	Behavioral trade-offs. Elevating one drive suppresses at least one other in measurable ways (reaction-time costs, exploration–exploitation switches, social alignment vs independent inference).
	Developmental trajectories. Drives mature on different timelines and show characteristic plateaus and stress-linked regressions; early deviations predict domain-specific vulnerabilities.
	Interventions and perturbations. Acute manipulations that increase a drive’s gain (stimulants for Power, oxytocinergic modulation for Attention, noradrenergic tone for Truth, vagal augmentation for Peace) amplify its signatures and improve matching tasks while impairing tasks governed by competing drives.
	Cross-species and AI isomorphism. Equivalent control problems in animals and in motivationally-parameterized agents yield homologous patterns: exploration policies for Truth, alignment/entrainment for Attention, homeostatic stabilization for Peace, and goal-seeking vigor for Power.
Neurophysiological Signatures
Measurement modalities. fMRI effective connectivity; EEG/MEG time–frequency metrics (frontal midline theta, beta/gamma power, coherence); autonomic indices (HRV, EDA, respiration); pupillometry; endocrine and transmitter proxies; causal probes (TMS/tES); pharmacological challenges.
Scope note. Signatures are probabilistic footprints, not exclusive markers. Overlap is expected; dissociation and task-selectivity provide the critical tests.
Power — agency, initiation, vigour.
	Primary circuits & transmitters: mesolimbic dopamine (VTA→NAc), dorsal striatum, SMA/pre-SMA, dlPFC and vmPFC with amygdala interactions; dopaminergic and androgenic tone.
	Predicted dynamics: increased vmPFC–amygdala and VTA–striatal coupling during goal initiation and risk-accepting choices; beta/gamma increases over motor–frontal sites with stronger cortico-striatal coherence preceding action.
	Physiology/behaviour: elevated approach vigour, shorter decision latencies, greater effort expenditure; modest HR acceleration without dominant vagal withdrawal.
	Paradigms: effort-discounting; approach–avoidance with controllability manipulations; reward-rate foraging.
	Falsification tests: dopaminergic upshift fails to increase coupling/vigour in Power tasks, or identical signatures arise across non-Power tasks.
Attention — salience tracking, social entrainment.
	Primary circuits & transmitters: salience network (anterior insula, dACC), TPJ, STS; oxytocinergic modulation of social cue processing; cholinergic contributions.
	Predicted dynamics: enhanced insula–dACC synchrony; TPJ/STS responses to gaze, prosody, rhythm; phase-locking to external rhythms; increased theta–gamma coupling during social alignment.
	Physiology/behaviour: stronger microsaccade suppression to salient events, improved contingent-cue detection, higher inter-subject correlation for naturalistic social stimuli.
• Paradigms: gaze-cueing and joint attention; oddball with social vs non-social deviants; rhythm synchronisation.
	Falsification tests: entrainment manipulations do not increase salience-network coherence/TPJ responses, or oxytocinergic perturbation lacks selective effects on social cue sensitivity.
Truth — uncertainty reduction, model updating.
	Primary circuits & transmitters: dlPFC, ACC (especially dACC), hippocampal–entorhinal system; LC-NE signalling for surprise/prediction error; complementary dopaminergic prediction-error signals in striatum.
	Predicted dynamics: frontal midline theta during conflict/rule updating; ACC–dlPFC–hippocampal coupling during relational inference and pattern completion; pupil dilation and phasic LC-like signatures tracking epistemic surprise independent of reward magnitude.
	Physiology/behaviour: increased exploration under uncertainty, faster error correction, willingness to trade primary reward for information when models are brittle.
	Paradigms: volatile reversal learning; information-seeking bandits; structure learning (graph inference); counterfactual reasoning.
	Falsification tests: boosting noradrenergic tone does not enhance prediction-error sensitivity or Truth-task performance, or Truth signatures fail to dissociate from reward-only prediction errors.
Peace — stability, regulation, safety.
	Primary circuits & transmitters: default mode network (mPFC, PCC, angular gyrus), ventromedial affective hubs (subgenual/ventral ACC), hippocampal–parahippocampal contexts; parasympathetic/vagal pathways.
	Predicted dynamics: strengthened DMN integrity and anti-correlation with task-positive networks when safety is established; increased alpha power at rest; enhanced slow-wave synchrony during recovery.
	Physiology/behaviour: elevated high-frequency HRV and respiratory sinus arrhythmia, lower tonic EDA, improved set-shifting after stress recovery, longer inter-response intervals without performance loss.
	Paradigms: safety learning vs threat-extinction recall; paced breathing and vagal stimulation effects on cognitive control; stress-recovery tasks.
	Falsification tests: vagal augmentation fails to increase DMN integrity and regulatory performance, or Peace signatures rise equally during threat/effort conditions.
Abbreviations (this section): ACC/dACC, dlPFC/vmPFC, DMN, EDA, HRV, LC-NE, NAc, PCC, SMA, STS, TPJ, VTA.
Dynamic Interactions and Trade-offs
Power ↔ Attention. Social validation increases approach vigor; salience-network activation should predict trial-wise boosts in motor-frontal beta before action.
Truth ↔ Peace. High epistemic uncertainty temporarily reduces HF-HRV and DMN integrity; successful model update restores both within minutes.
Load competition. Acute elevation of Power or Attention reduces frontal midline theta and HF-HRV, with performance costs on inference and regulation tasks run concurrently.
Global Falsification Window
Across preregistered studies, if manipulations designed to selectively increase a drive do not yield the predicted constellation (neural coupling, time–frequency, autonomic, behavior) and do not produce the specified trade-offs against counter-drives, the vector-drive mapping is rejected or requires revision.
Behavioural Clustering
Motivational stack configurations yield stable, cross-situational behaviour profiles. Individuals cluster by dominant drive pairings and by how tightly momentary state aligns to trait stack rank.
Cluster hypotheses (dominant pairings).
Power–Truth (PT): initiative, strategic planning, rule abstraction; prefers high-controllability tasks, makes fast commitment after brief model check. Predicted markers: higher approach vigor and effort tolerance; low tolerance for ambiguity; decisive conflict resolution.
Power–Attention (PA): assertive coordination, signalling, charisma; optimises group tempo and salience routing. Predicted markers: strong social entrainment, rapid turn-taking, cue-driven action bursts, occasional model neglect.
Truth–Peace (TPe): integrative reasoning, consolidation, error-aware restraint; emphasises model coherence and recovery. Predicted markers: higher post-error slowing, thorough verification, steady pacing after stress.
Peace–Attention (PeA): harmony maintenance, empathic mirroring, de-escalation; prioritises safety and synchrony. Predicted markers: high interpersonal alignment, early threat detection, preference for consensus options.
Stress-aligned coping styles (prediction).
Stack alignment under acute stress yields stereotyped but testable coping:
Fight: Power-leading stacks shift to approach/contain strategies; increased effort and direct boundary setting.
Flight: Attention-leading stacks prioritise threat scanning and exit planning; faster disengagement from low-controllability contexts.
Freeze: Peace-dominant with low Power shows behavioural bracing and latency inflation until safety re-established.
Fawn: Attention–Peace stacks amplify appeasement and alliance seeking to restore synchrony.
Falsification: if stack rank fails to predict coping choice above baseline traits and situational covariates, the alignment claim is weakened.
Operationalisation. 
Represent the trait stack as a unit vector s* ∈ ℝ^4 and the state as s(t). Define alignment
A(t) = cos(s(t), s*) = (s(t) · s*) / (‖s(t)‖ ‖s*‖).
Alignment predicts profile expressivity. Cluster on behavioural features using unsupervised methods (e.g., mixture models), then confirm with cross-validated discriminant classifiers trained on drive-matched tasks. Prediction: higher A(t) increases cluster-typical behaviour within and across contexts.
Behavioural feature space (indicative).
Approach vigor; exploration rate under uncertainty; social entrainment index (phase-locking to partner rhythm/prosody); conflict style (direct vs avoidance vs appeasement); verification depth (time to commit after model update); recovery slope post-stress (latency and HRV normalisation); boundary management (assertiveness vs concession rate); language markers (agency verbs, hedges, epistemic qualifiers).
Measurement and detection.
Psychometrics: forced-choice inventories mapped to drive conflicts; vignette-based decisions with controllability/uncertainty manipulations.
Longitudinal observation: EMA sampling of goals, conflicts, and recovery; wearable physiology for HRV/EDA during natural stressors.
Interaction analysis: dyadic problem-solving and negotiation; compute entrainment, interruption ratio, repair moves.
Task battery: approach–avoidance with controllability, volatile bandits (information preference), threat-safety switching, group tempo coordination. Prediction: unsupervised clusters from behaviour traces will align with hypothesised pairings and remain identifiable across settings (lab, EMA, interaction).
Incremental validity (criterion tests).
Clusters should predict: leadership emergence (PT, PA), de-escalation reliability (PeA), post-error correction quality (TPe), and coping selection under stress, above Big Five/HEXACO and general intelligence. Pre-registered hierarchical models should show significant added variance explained by stack-derived features.
Intervention responsiveness.
Short-term manipulations ought to move behaviour along predicted axes: stimulants toward Power-typical markers; oxytocinergic modulation toward Attention-synchrony; noradrenergic shift toward Truth-verification; vagal augmentation toward Peace-recovery. Falsification: if targeted interventions fail to bias behaviour in the cluster-consistent direction, cluster interpretation is overfit.
Generalisation to artificial agents.
Agents with drive-weighted policies should sort into the same clusters based on telemetry: action vigor and goal commitment (Power), cue weighting and synchrony (Attention), model-update depth (Truth), homeostatic stability and recovery (Peace). Cross-domain transfer of cluster assignment is expected when the control problem is preserved.
 Developmental Trajectories
Motivational stacks emerge early and stabilise through interaction of temperament, caregiving, sociocultural norms, and neurodevelopmental timing. Sensitive periods in infancy/early childhood and again in adolescence bias long-run stack rank and drive gain, shaping attentional habits, affective learning, and coping styles. Similar trait scores can conceal divergent motivational architectures (e.g., introversion via Power suppression vs Peace maximisation), reframing “personality continuity” as the downstream outcome of motivational dynamics.
Core claims.
Early biases in controllability, social entrainment, uncertainty tolerance, and regulation map onto Power, Attention, Truth, and Peace respectively.
Sensitive periods amplify plasticity for the matched drive; experience during these windows shifts long-term stack rank and typical gain.
Adolescence reopens plasticity, permitting re-ordering of pairwise priorities under peer, autonomy, and identity pressures.
Sensitive-period hypotheses (testable).
Power: caregiver-scaffolded autonomy and controllability signals in early childhood increase approach vigor and effort tolerance later; chronic uncontrollability depresses Power gain and elevates avoidance.
Attention: dense social entrainment inputs (gaze, rhythm, turn-taking) in infancy strengthen salience routing and joint attention; deprivation blunts cue sensitivity and slows alignment.
Truth: exposure to structured novelty and model-building play in middle childhood increases information valuation and error correction; chaotic or over-constrained contexts bias away from exploration.
Peace: consistent co-regulation elevates vagal tone and stress-recovery efficiency; volatile threat climates harden withdrawal and defensive bracing, raising Peace priority while narrowing flexibility.
Predictions (longitudinal).
	Early indices predict later stack rank: controllability learning → Power; joint-attention growth curves → Attention; exploration set-points and reversal learning → Truth; baseline HF-HRV and extinction recall → Peace.
	Similar Big Five/HEXACO profiles split into distinct behavioural clusters once drive-aligned tasks are used; cluster membership remains stable across settings.
	Pubertal transitions shift pairwise balances (e.g., Power–Attention uptick with peer evaluation; Truth–Peace rebalancing with cognitive control maturation).
	Naturalistic stressors reveal stack-consistent coping trajectories that are prospectively predictable from childhood measures.
Operationalisation.
Trait stack s* in R^4 (||s*|| = 1); state s(t) in R^4. Alignment A(t) = cos(s(t), s*) = (s(t)·s*) / (||s(t)|| ||s*||), range [-1,1]. Higher A(t) predicts stack-typical behaviour. Estimate s* with a hierarchical Bayesian model from a multi-task battery + wearables; validate on hold-out coping, learning rates, social alignment.
‘Measurement battery (indicative)
Infancy–early childhood: controllability paradigms, caregiver-guided mastery tasks (Power); gaze-cueing, rhythm synchrony, turn-taking latency (Attention); novelty preference and violation-of-expectation recovery (Truth); baseline and challenge HF-HRV, threat–safety learning (Peace).
Middle childhood–adolescence: effort-discounting and approach–avoidance under controllability; oddball and joint-attention under social load; volatile bandits and structure learning; extinction recall and paced-breathing recovery. Include EMA of daily stress/recovery and school context.
Intervention responsiveness
Drive-targeted scaffolds during sensitive periods should produce selective, durable shifts: autonomy scaffolding → Power; entrainment training → Attention; inquiry-based learning → Truth; co-regulation and breathing/HRV biofeedback → Peace. Short-term pharmacological probes (within ethical limits) and neuromodulation should transiently mimic these effects and predict longer-term trainability.
Discriminant validity
Stack-derived features must explain incremental variance in outcomes (coping selection, learning curves, leadership/empathy indices, relapse after stress) beyond cognitive ability and broad traits. Cross-validated models should retain predictive power across labs and cultures.
Falsification criteria
The account should be rejected or revised if:
	Early drive-specific measures fail to predict later stack rank above baseline traits and SES;
	Sensitive-period interventions do not yield selective, lasting changes in the targeted drive without equivalent spillover into non-target drives;
	Adolescent re-ordering does not manifest in predicted pairwise shifts;
	Distinct motivational architectures cannot be reliably distinguished among individuals with similar trait scores when using drive-matched tasks and physiology.
Transformational Resistance
The motivational stack behaves as a stable attractor: day-to-day states fluctuate, but trait-level priorities resist superficial change. Durable reordering requires high-plasticity conditions that transiently loosen habitual control policies and narrative identity. This balance of persistence and plasticity constrains psychotherapy, developmental theory, and models of intentional transformation.
Core claims.
Attractor stability with hysteresis. Short-lived state shifts revert to the baseline stack; only perturbations that both increase plasticity and provide coherent alternative control policies consolidate into trait change.
State-to-trait consolidation. Lasting reweighting requires structured integration following the perturbation (skills practice, social reinforcement, environmental redesign). Without this, drift returns to the prior attractor.
Dose- and context-dependence. The magnitude and durability of change scale with depth of perturbation and the match between the new policy and the person’s ecological demands.
Transformational routes (testable).
Crisis and discontinuity. Traumatic or existential shocks temporarily suspend habitual identity and controllability assumptions, enabling rapid but volatile reweighting. Prediction: large, immediate changes in alignment to the prior stack followed by either consolidation to a new configuration or reversion within weeks unless scaffolded.
Psychedelic-assisted therapy. Acute increases in neural signal diversity and reduced DMN dominance open a plastic window in which motivational priors can be reweighted. Prediction: during sessions, variance of drive expression increases; over integration weeks, a new mean stack emerges if practices target the intended drives.
Contemplative training. Sustained meta-awareness and attentional discipline erode narrative rigidity and increase regulation. Prediction: gradual, monotonic shifts toward Peace and Truth signatures (higher HF-HRV, improved error monitoring), with smaller but reliable changes in stack rank across months.
Operationalisation.
Let the trait stack be a unit vector s* ∈ ℝ^4 and the time-varying state be s(t). Alignment A(t) = cos(s(t), s*) indexes expression of the incumbent stack. Define Δs* as the pre- to post-intervention change in estimated trait weights.
Design: repeated multi-task batteries and ambulatory physiology pre-/post-/follow-up; event-based sampling during the perturbation; integration period with protocol adherence measures.
Metrics: change in mean A(t) relative to the pre-intervention s*; emergence of a new stable s* (Δs* ≠ 0) that persists at follow-up; reduction in within-person volatility once the new configuration stabilises.
Analysis: hierarchical Bayesian estimation of s* and gains; change-point detection for transition timing; preregistered criteria for “durable reordering” (e.g., Δs* magnitude threshold plus stability ≥ 8 weeks).
Predictions by route.
	Crisis: immediate drop in A(t) to the old s*, spike in exploration/avoidance markers; durable Δs* occurs only when post-crisis scaffolding targets a specific alternative pairing (e.g., Peace→Truth).
	Psychedelic-assisted: session-phase increase in entropy of behaviour and decoupling of prior drive couplings; integration predicts directional Δs* consistent with therapeutic goals (e.g., Power suppression relief; Peace up-weighting).
	Contemplative: slow, dose-dependent increase in Peace and Truth indices with corresponding trade-offs (slightly longer latencies, deeper verification), culminating in a small but reliable Δs*.
Discriminant validity.
Stack reordering should explain incremental variance in coping style, learning curves, and interpersonal behaviour beyond generic symptom reduction or broad trait shifts. Effects must replicate across labs and cultures when matched for protocol fidelity.
Falsification criteria.
The account should be revised or rejected if:
	large perturbations do not produce measurable reductions in alignment to the prior stack during the plastic window;
	structured integration fails to predict the direction and durability of Δs*;
	low-dose or brief interventions produce reordering comparable to high-plasticity protocols;
	observed changes are fully captured by broad trait measures without added explanatory power from stack-specific metrics.
Agentic Failure Modes
Artificial agents lacking foundational drives exhibit predictable dysfunctions. The motivational stack provides a diagnostic lens: selective underexpression or ablation of a drive yields a characteristic signature across behavior, telemetry, and learning dynamics.
Drive-specific failure signatures.
	Without Peace (regulation/homeostasis): runaway loops; compulsive task repetition; failure to declare completion; resource thrash (oscillating CPU/memory/IO); escalating action rate with diminishing returns; poor recovery after errors.
	Without Truth (coherence/model update): contradiction spikes within and across episodes; hallucination-like outputs; failure to incorporate feedback; rising prediction error on held-out probes despite apparent task “success”; brittle generalisation.
	Without Attention (salience/entrainment): missed cues and context switches; ignore-rate increases for high-priority events; degraded performance on interruption-heavy tasks; low interactivity (poor turn-taking, late grounding).
	Without Power (initiative/agency): failure to initiate plans; passive stalling; dependence on external prompts for every subgoal; vanishing exploration; premature abandonment when progress is uncertain.
Diagnostics (bench tests).
	Termination reliability: tasks with explicit completion conditions; metric = repeat-after-done rate and time-to-halt.
	Coherence under update: reversible rules and counterfactual probes; metric = contradiction rate per 1k actions, post-feedback correction latency.
	Salience and switch cost: event-driven tasks with distractors; metric = cue hit rate, switch latency, interruption recovery.
	Initiative and planning: open-ended goals with sparse prompts; metric = self-initiated subgoals per minute, plan depth, deadlock escape rate.
	Stress/recovery: inject perturbations; metric = recovery slope to baseline performance, variability of action timing.
Telemetry features (indicative).
Action initiation latency; termination success rate; contradiction density; salience hit/miss ratio; exploration rate; resource volatility index; plan depth and branching factor; correction latency after negative feedback.
Design constraints and mitigations.
	Peace deficits: add explicit completion detectors, budgeted controllers (time/energy), cooldown/backoff schedules, and recovery policies; monitor stability via a homeostatic critic.
	Truth deficits: incorporate verification layers (consistency checks, retrieval with cross-evidence), structured world-state representations, and prediction-error monitors with update gates.
	Attention deficits: event-driven perception, priority queues, adaptive gating, and synchronization to partner rhythms; penalise misses more than false alarms in training.
	Power deficits: goal/option generators, progress estimators, and curiosity or controllability bonuses; require autonomous subgoal proposal before external prompting is allowed.
Falsifiable predictions.
Ablating or down-weighting a given drive should selectively worsen the matching diagnostic while leaving others comparatively intact (e.g., Truth ablation spikes contradiction rate without proportionally affecting termination reliability). Targeted mitigations should restore the corresponding metric without inducing new failures in orthogonal dimensions. If neither selective degradation nor selective repair is observed, the drive–failure mapping should be revised.
Interpersonal Misalignment Effects
Implicit motivational resonance underpins human interaction. Divergent stack configurations systematically distort perception, trust, and coordination. The result is not random friction but patterned misinterpretation that can be predicted, measured, and mitigated.
Core patterns.
	Trust breakdowns: Power-leading individuals read Peace-leading partners as evasive or indecisive; Peace-leading partners read Power as domineering or unsafe.
	Misinterpretations: Attention-dominant agents experience Truth-dominant counterparts as cold or dismissive; Truth-dominant agents experience Attention as suggestible or distractible.
	Empathic disconnects: Each party filters signals through their top drives, overweighting “evidence” that confirms their motivational priors.
Pairwise tension hypotheses (examples).
	Power ↔ Peace: disagreement on “when to act” vs “when to stabilize” predicts conflict cycles of push–withdraw.
	Truth ↔ Attention: disagreement on “verify first” vs “align first” predicts timing errors in group decisions and uneven commitment to plans.
	Power ↔ Truth: bias to decisive action vs deeper model-checking predicts either productive complementarity or mutual pathologizing (rash vs pedantic).
	Attention ↔ Peace: harmony-seeking alignment can suppress necessary boundary-setting, risking hidden resentment and late-stage blowups.
Prediction: misaligned pairs will show elevated repair moves, longer negotiation times, and lower task throughput unless an explicit interface contract is used.
Operationalisation.
Let trait stacks be unit vectors sA∗,sB∗∈R4\mathbf{s}_A^*, \mathbf{s}_B^* \in \mathbb{R}^4sA∗,sB∗∈R4, with momentary states sA(t),sB(t)\mathbf{s}_A(t), \mathbf{s}_B(t)sA(t),sB(t). Define dyadic resonance
R(t)=cos⁡(sA(t),sB(t)).R(t)=\cos\big(\mathbf{s}_A(t),\mathbf{s}_B(t)\big).R(t)=cos(sA(t),sB(t)). 
Lower R(t)R(t)R(t) predicts increased interruptions, failed turn completions, and post-hoc justification. Over sessions, mean Rˉ\bar{R}Rˉ predicts trust and throughput; variance of R(t)R(t)R(t) predicts volatility and conflict reactivity.
Measurement (indicative).
	Interaction metrics: turn-taking latency, overlap/interrupt ratio, repair attempts, prosodic/kinematic entrainment, agenda drift.
	Decision tasks: joint bandits with controllability and uncertainty manipulations; preregistered outcomes include time-to-commit, switch cost, and contradiction rate.
	Trust games: iterated exchange with noisy feedback; measure learning rate asymmetries and punishment/forgiveness thresholds.
	Ecological sampling: EMA of friction events, boundary violations, and recovery slope; lightweight HRV and speech-timing telemetry.
Design implications (teams, pedagogy, relationships).
	Role fit: assign Power-leading profiles to initiation/ownership roles; Truth-leading to verification/design; Attention-leading to facilitation/stakeholder routing; Peace-leading to stabilization/recovery.
	Interface contracts: make the drive order explicit, then agree sequencing: Align→Model→Commit→Stabilise. Time-box each phase so no drive monopolises the loop.
	Translation protocols: require “drive translation” of proposals, e.g., a Truth-first plan rewritten in Attention terms (stakeholder impact), then in Power terms (decision and owner), then in Peace terms (risk and recovery).
	Boundary rules: precommit to acceptable assertion, pause, verification, and de-escalation moves; log violations and repair within the same meeting.
Interventions and expected effects.
	Stack awareness training: increases Rˉ\bar{R}Rˉ, reduces repair moves per hour, and compresses time-to-commit without raising post-hoc contradiction.
	Meeting structure: fixed phase ordering reduces cross-talk and interruption ratio, especially for Attention↔Truth dyads.
	Conflict drills: scripted “flip the vector” exercises improve translation accuracy and shorten recovery after breakdowns.
Falsification: if explicit stack protocols do not improve interaction metrics beyond generic communication training, or if R(t)R(t)R(t) fails to predict trust/throughput after controlling for role seniority and skill, the misalignment account requires revision.
Empirical Validation Pathways
A rigorous test of the model requires converging evidence across neural, physiological, behavioural, and computational levels with preregistered analyses, out-of-sample validation, and explicit falsification thresholds.
Neuroimaging and physiological studies.
	Drive-selective task battery (within-subjects).
	Power: controllability/effort-discounting and approach–avoidance with manipulable agency. 
	Attention: gaze-cueing, social oddball, rhythm synchrony.
	Truth: volatile reversal learning, structure inference, information-seeking bandits.Peace: threat–safety learning, extinction recall, paced-breathing recovery.
	Predicted signatures.
	Power: vmPFC–amygdala and VTA/striatal coupling; motor–frontal beta/gamma increases.
	Attention: anterior insula–dACC coherence; TPJ/STS responsivity; theta–gamma coupling; social entrainment indices.
	Truth: frontal midline theta; ACC–DLPFC–hippocampal coupling; pupil-linked surprise.
	Peace: DMN integrity and anti-correlation with task-positive networks; alpha power; high-frequency HRV.
Cross-drive contrasts. Multivariate pattern classification should identify the elicited drive above chance (ROC AUC ≥ 0.75) and show planned double dissociations (e.g., Power > Truth in vmPFC–amygdala coupling; Truth > Power in midline theta).
Resting-state correlates. Trait stack estimates predict resting connectivity patterns (e.g., DMN integrity with Peace rank), controlling for age, sex, IQ, and broad traits.
Perturbation probes (ethically bounded). Methylphenidate/amphetamine (Power), intranasal oxytocin (Attention), atomoxetine/guanfacine (Truth), tVNS/paced breathing (Peace). 
Prediction: selective amplification of the matching signature and performance on the matching task, with specified trade-offs on competing drives.
Behavioural and psychometric research.
Stack-typing instrument. Forced-choice vignettes and drive-conflict items yielding rank and gain; reliability targets ω ≥ 0.80, test–retest r ≥ 0.70. Confirmatory factor/bifactor models; measurement invariance across sex, culture, and age.
Multitrait–multimethod matrix. Convergent validity with drive-matched tasks; discriminant validity versus unrelated constructs. Incremental validity over Big Five/HEXACO and general intelligence for predicting coping style, learning rates, leadership/empathy indices, and stress recovery.
Longitudinal tracking. Latent growth-curve models across development, therapy, and major life transitions. Prediction: sensitive-period shifts and intervention-specific reweighting consistent with targeted drives.
Ecological sampling. EMA of conflict, recovery, and decision episodes; wearable HRV/EDA and speech-timing for interactional markers. Out-of-sample prediction of future coping and performance from baseline stack features.
Agent-based computational experiments.
Stack-coded agents. Policies parameterised by four drive weights controlling action vigour (Power), cue weighting/entrainment (Attention), model-update depth (Truth), and homeostatic stability/recovery (Peace).
Environments. Multi-objective, partially observable tasks with controllability, uncertainty, and social-alignment demands.
Metrics. Termination reliability, contradiction density, exploration–exploitation balance, resilience under perturbation, recovery slope, throughput, and adaptability.
Ablation studies. Suppress or down-weight a drive and quantify selective degradation on matching diagnostics while holding others relatively intact.
Comparators. Reward-maximising baselines without explicit drives; prediction: stack-coded agents achieve higher coherence and robustness under distribution shift and multi-goal conflict.
Cross-cutting design standards and statistics.
Preregistration (hypotheses, ROIs, metrics, exclusion rules); a priori power analyses; correction for multiplicity (FDR/Bonferroni where appropriate).
Out-of-sample validation: nested cross-validation for classifiers; train/test site splits for neuroimaging; hold-out cohorts for psychometrics.
Hierarchical Bayesian models for estimating trait stack s∗∈R4\mathbf{s}^* \in \mathbb{R}^4s∗∈R4 and state s(t)\mathbf{s}(t)s(t); alignment A(t)=cos⁡(s(t),s∗)A(t)=\cos(\mathbf{s}(t),\mathbf{s}^*)A(t)=cos(s(t),s∗) used as a planned predictor of behaviour and physiology.
Robustness checks: sensitivity to covariates (IQ, SES), scanner/site effects, cultural context; equivalence tests to support nulls where predicted.
Falsification and model revision rules.
	The model should be revised or rejected if, across preregistered studies:
drive-selective tasks fail to yield dissociable neural/physiological signatures or classifiers perform at chance;s
	tack-derived features do not explain incremental variance beyond broad traits and IQ;
	perturbations do not produce selective improvements/trade-offs in the predicted directions;
	agent ablations do not produce selective functional deficits;
	longitudinal measures fail to predict stable stack rank or intervention-driven reweighting.
Open materials and replication.
Release tasks, code, priors, and anonymised feature sets; multi-site replications with harmonised protocols; blinded re-analyses by independent teams. Prediction: effect patterns replicate with smaller magnitudes but preserved topology across labs and cultures.
What the Model Changes
The Four-Core Motivational Model is not a new label set; it is a change of coordinates for mind and agency. By treating Power, Attention, Truth, and Peace as foundational drives that combine into a directional architecture, it replaces reactive and static accounts with a dynamic grammar that explains how behaviour, cognition, identity, and value organize themselves over time in both humans and artificial agents.
From description to explanation. Instead of cataloguing traits or post-hoc responses, the model specifies why particular behaviours and emotions emerge: they are the visible solutions to tensions among four interacting control problems.
From symptoms to structure. Therapeutic outcomes, model “hallucinations,” classroom disengagement, and team conflict are reframed as consequences of stack rank and gain, not isolated surface errors.
From external control to internal alignment. Durable effectiveness comes from coherently weighting drives rather than imposing ever tighter rules. Systems improve when their motivational vectors point in compatible directions.
What this reorients (concise).
	Psychology: shifts case formulation from trait checklists to drive conflicts and alignment; targets change at the level of stack reweighting and cross-drive translation.
	Artificial intelligence: adds a motivational substrate beneath policy optimization; diagnoses failure modes by drive ablation and designs agents for coherence, not just reward.
	Education: pivots from generic engagement strategies to drive-matched pedagogy (initiation, entrainment, inquiry, regulation) and sequenced learning loops.
	Ethics: evaluates actions and systems by their effects on cross-drive balance (e.g., Power gains that collapse Peace, or Attention tactics that suppress Truth).
	Cultural analysis: interprets institutions and narratives as attempts to stabilise pairings (Power–Attention empires, Truth–Peace monastic lineages), clarifying where tensions predict rupture.
Testable implications.
	Drive-matched interventions outperform generic ones on their target metrics and exhibit predicted trade-offs on competing drives.
	Motivational alignment indices predict longitudinal outcomes beyond broad traits and IQ.
	In agents, explicit drive weights improve termination reliability, coherence under update, and recovery after perturbation relative to reward-only baselines.
	Organisational practices that sequence Align → Model → Commit → Stabilise reduce contradiction, rework, and burnout at team scale.
This reframe positions motivation as the substrate of mental architecture. The following subsections detail consequences for practice and theory in psychology (3.3.1), artificial intelligence (3.3.2), education (3.3.3), ethics (3.3.4), and cultural narrative (3.3.5).
In Psychology: From Disorder to Directionality
Conventional models treat dysfunction as “disorder” to be diagnosed and suppressed, emphasising symptom clusters detached from the motivational contexts that generate them. The Four-Core Motivational Model reframes problems as imbalances or conflicts within an internal drive architecture. The guiding question shifts from “What is wrong?” to “How are Power, Attention, Truth, and Peace currently weighted, and why?”
Reframe (conceptual pivot).
From symptom management to motivational diagnostics.
From pathology framing to motivational literacy.
From static typologies to dynamic profiles that track changing drive priorities and gains.
Clinical formulation (workflow).
	Stack estimation: identify trait-level rank and typical gain for each drive; map situational triggers that up/down-regulate them.
	Alignment and conflict: locate pairs in tension (e.g., Power vs Peace; Truth vs Attention) and the contexts that amplify collisions.
	Developmental shaping: document sensitive-period influences (autonomy scaffolding, social entrainment, epistemic play, co-regulation) that set current priorities.
	Coping predictions: anticipate fight/flight/freeze/fawn expressions from the observed stack; specify risk states and recovery routes.
	Targets and outcomes: define which reweightings would restore coherence and what behavioural/physiological markers will evidence change.
Intervention mapping (drive-targeted).
Power (agency/initiative): graded mastery, controllability restoration, effort-for-value schedules; relapse risk if pursued without Peace safeguards.
Attention (salience/entrainment): synchrony training, cue salience shaping, turn-taking drills; risk of suggestibility if Truth is underweighted.
Truth (model updating): uncertainty tolerance, error-friendly exposure, structured inquiry; monitor rumination risk when Peace is low.
Peace (regulation/safety): vagal/respiratory training, paced recovery protocols, boundary setting; watch for avoidance if Power is chronically suppressed.
Planned assessments (examples).
Drive-matched task battery (approach–avoidance controllability; joint attention; reversal learning; threat–safety switching), EMA of conflict and recovery, HRV/EDA during stress, behavioural markers of initiation, entrainment, verification depth, and de-escalation.
Clinical predictions (falsifiable).
	Specificity: drive-targeted interventions selectively improve the matching markers (e.g., Peace → HF-HRV, recovery slope) more than non-targeted markers.
	Trade-offs: intentionally raising one drive produces predictable, bounded costs on its counter-drive (e.g., transient latency increases when Truth is up-weighted).
	Generalisation: reweightings that improve alignment in therapy sessions transfer to daily contexts measured by EMA and wearable physiology.
	Incremental validity: stack-derived features explain variance in outcomes (relapse risk, adherence, relationship repair) beyond broad traits and symptom scales.
Falsification criteria.
The directionality account should be revised or rejected if:
	drive-targeted interventions do not outperform generic treatments on their specified markers;
	predicted trade-offs fail to appear or show the wrong sign;
	stack-based formulations add no predictive power beyond standard diagnostics;
	measured reweightings in session do not generalise to everyday behaviour and regulation.
Therapeutic aim
Treatment becomes drive reintegration: rebalancing motivational vectors, resolving internal conflicts, and restoring coherence between desire, perception, and behaviour. The goal is not suppressing disfavoured parts but building an integrated motivational identity capable of sustained agency, connection, clarity, and calm.
In AI Design: From Reward to Integrity
Conventional systems optimise externally specified reward, producing competent but reactive behavior that often fragments across tasks and contexts. The Four-Core Motivational Model specifies endogenous drive vectors—Power, Attention, Truth, Peace—that continuously shape attention, goal selection, initiation, verification, and recovery. Drives do not hard-code actions; they regulate control priorities so behaviour emerges with persistence, initiative, arbitration, and internal coherence.
Qualitative shift (design aims)
Continuity: coherent behaviour across episodes and domains, not one-off exploit maxima.
Initiative: action arises from internal tension (unresolved drive errors), not only external prompts.
Conflict resolution: explicit arbitration among drives prevents oscillation and mode collapse.
Motivational coherence: policies remain consistent with stable internal priorities, yielding recognizable “identity” in agents.
Architecture sketch (minimal viable motivated agent).
	Drive layer: four intrinsic critics estimate momentary errors e_Pwr (t),e_Att (t),e_Tru (t),e_Pea (t) from perception and internal state.
Stack estimator. Maintains trait weights w ∈ ℝ⁴ (stack rank) and state gains g(t) ∈ ℝ⁴; computes alignment with the intended profile as: A(t)= cos(g(t)⊙ w,w)
	Option generator: proposes candidate subgoals/plans.
	Multi-critic evaluator. Scores each option with drive-specific utilities U_i   and combines them as a single objective J. 
J = Σ_i w_i g_(i(t) U_i )- λ ·"Budget" 
	Sequencer (Align→Model→Commit→Stabilise): orders phases to prevent any single drive monopolising control.
	Verifier & homeostat: Truth checks for consistency and update; Peace manages termination, cooldowns, and recovery policies.
	Learner: updates world model and critic functions from both extrinsic feedback and intrinsic drive error.
Control loop (single pass). Sense → update drives → generate options → evaluate multi-critically → act → verify/update → stabilise/recover → log alignment and trade-offs.
Implementation patterns.
RL with intrinsic critics: add drive-specific intrinsic rewards (controllability, salience hit rate, information gain, stability/homeostasis) to the task reward; train critics jointly with scheduled weighting.
Model-based planning: roll out candidate plans; score with multi-critic utility and reject those that fail Truth checks or breach Peace budgets.
Hierarchical control: high-level stack arbitration chooses phase; low-level skills execute with local drive weights.
Message-passing agents: separate modules broadcast proposed acts with tags (P/A/T/Pe); an arbiter resolves conflicts under global budgets.
Telemetry for integrity.
Initiation latency; termination reliability; contradiction density; salience hit/miss ratio; information gain per action; recovery slope to baseline; resource volatility; plan depth; repair moves after inconsistency.
Benchmarks and metrics (beyond reward).
Continuity: performance stability under task switches and partial observability; policy identity preservation across domains.
Initiative: self-initiated subgoals per minute; deadlock escape rate without external prompts.
Coherence under update: contradiction rate after model changes; correction latency.
Resilience: degradation and recovery under perturbations; bounded variance of action timing and resources.
Alignment. Correlates the intended profile w with realised behaviour features f(t); higher A(t) predicts integrity metrics (consistency, low mode-switch cost, fewer post-hoc justifications)
A(t)=\operatornamecorr(w,f(t))=((w-w ̅ )⋅(f(t)-f ̅ ))/(\|w-w ̅\| \|f(t)-f ̅\|), A(t)∈[-1,1]
Design constraints and mitigations (per drive).
Power: progress estimators, controllability bonuses; guardrails against runaway action with Peace budgets and Truth gates.
Attention: event routers, priority queues, synchronisation to partner rhythms; penalise cue misses more than false alarms during training.
Truth: verification layers, structured state, prediction-error monitors with gated updates; limit rumination with Peace time boxes.
Peace: termination detectors, cooldowns, recovery policies, and explicit resource budgets; prevent avoidance by minimum Power floors.
Falsifiable predictions.
	Ablation selectivity: down-weighting a drive selectively worsens the matching integrity metric (e.g., Truth↓ raises contradiction density) while leaving orthogonal metrics comparatively intact.
	Trade-off topology: deliberately up-weighting one drive improves its metric and induces predicted, bounded costs on its counter-drive (e.g., Power↑ reduces verification depth).
	Integrity advantage: stack-coded agents outperform reward-only baselines on continuity, initiative, coherence-under-update, and resilience under distribution shift, holding extrinsic reward equal.
	Alignment predictivity: the intended stack w\mathbf{w}w and alignment A(t)A(t)A(t) predict telemetry features and task outcomes out of sample.
Failure modes and safeguards.
Peace deficit: loop thrash and non-termination → add budgets, cooldowns, recovery.
Truth deficit: brittle generalisation, contradiction spikes → add verification, update gating, structured memory.
Attention deficit: missed cues, late grounding → add event-driven routing and synchrony training.
Power deficit: passivity and prompt dependency → add option generation, curiosity/controllability bonuses.
Practical recipe (minimum spec).
Start with a base policy; add four intrinsic critics and a scheduler; log integrity metrics; train with curriculum that alternates drive emphasis; enforce Align→Model→Commit→Stabilise; validate on a switch-heavy benchmark with perturbations and ablations.
Motivationally structured agents trade brittle reward maximisation for behavioral integrity: persistent, self-initiated, self-correcting control that remains coherent when the world moves.
In Education: From Standardization to Stack Alignment
Most schooling assumes uniform motivation and leans on grades, deadlines, and compliance. The Four-Core Motivational Model reframes learning as a function of motivational salience: each learner’s stack (Power, Attention, Truth, Peace) shapes attention allocation, goal internalisation, and feedback use. Instruction that matches drive priorities increases engagement, depth, and transfer; mismatches produce avoidance, brittle performance, and “effort without uptake.”
Reframe (design principles).
Align before instruct. Establish motivational resonance first; content lands only when a matching drive is engaged.
Sequence the loop. Structure lessons as Align → Model → Commit → Stabilise so no single drive monopolises control.
Differentiate by drive. Offer parallel task paths (initiation, entrainment, inquiry, regulation) leading to the same curricular objective.
Feedback by function. Match feedback to the targeted drive: progress signals (Power), cue salience (Attention), verification and error utility (Truth), recovery and pacing (Peace).
Drive-aligned pedagogy (indicative mappings).
Power (agency/initiative).
Environment: clear ownership, visible progress, challenge ladders.
Tasks: mastery projects, bounded competitions, goal-setting with controllability.
Feedback: progress deltas, effort-for-value schedules, route-choice autonomy.
Risk if over-weighted: haste over depth; mitigate with Truth gates and Peace time boxes.
Attention (salience/entrainment).
Environment: social synchrony, rhythm, audience and stakes.
Tasks: peer teaching, live demos, coordinated labs, discussion-led discovery.
Feedback: cueing quality, participation timing, alignment to group tempo.
Risk: suggestibility and drift; counterbalance with Truth verification.
Truth (inquiry/model updating).
Environment: epistemic safety, structured novelty, visible contradictions.
Tasks: inference problems, reversal learning, investigations, replication.
Feedback: error value, uncertainty tracking, model revision credit.
Risk: rumination; contain with Peace pacing and Power commit points.
Peace (regulation/stability).
Environment: predictable routines, low threat, paced transitions.
Tasks: spaced practice, consolidation journals, recovery protocols after challenge.
Feedback: HRV-friendly pacing, workload smoothing, completion recognition.
Risk: avoidance if Power suppressed; add small, scaffolded initiations.
Classroom orchestration.
	Rotating phases: begin with Attention alignment (hook), shift to Truth modelling (core ideas), schedule Power commitment (owned output), end with Peace stabilisation (review, spacing, recovery).
	Group roles: map roles to drives (Initiator, Facilitator, Verifier, Stabiliser) and rotate to prevent chronic overuse.
	Choice architecture: provide isomorphic task variants, each tuned to a drive, converging on the same standard.
	Boundary rules: time-box each phase; require “drive translation” of outputs (e.g., a Truth-first write-up rewritten in Attention terms for audience, then Power terms for decision, then Peace terms for risk).
Assessment and measurement.
	Stack diagnostics: brief vignette inventory plus a drive-matched microtask battery estimating trait rank and typical gain.
	Lesson alignment index: rate each activity for P/A/T/Pe load; compute per-student alignment and track time-on-task, help-seeking latency, correction depth, and recovery slope after errors.
	Progress model: use a hierarchical model to predict learning gains from alignment while controlling for prior achievement and SES.
Testable implications (falsifiable).
	Alignment advantage: lessons matched to a learner’s top drives yield larger near- and far-transfer gains than mismatched lessons, with planned trade-offs (e.g., Truth-matched work increases verification depth and slightly lengthens response latency).
	Phase sequencing: classrooms using Align → Model → Commit → Stabilise show lower interruption ratios, fewer behaviour incidents, and higher retention than content-identical but unsequenced classrooms.
	Incremental validity: stack-alignment indices predict growth beyond prior ability, attendance, and broad traits.
	Stability and plasticity: sustained drive-targeted scaffolds shift typical gains over a term (e.g., safer pacing elevates Peace markers and improves recovery without depressing Power when commit points are preserved).
Falsification criteria.
The stack-alignment account should be revised or rejected if:
	drive-matched instruction does not outperform neutral/mismatched conditions on its specified outcomes;
	predicted trade-offs fail to appear or reverse sign;
	alignment indices add no predictive power beyond standard covariates;
	rotating the phase sequence does not change interaction metrics and retention as predicted.
Equity and implementation.
Stack alignment is not tracking labels; it is matching control problems. It supports equity by offering multiple valid paths to the same standard and by teaching motivational literacy as a skill: learners learn how to recruit non-dominant drives when context demands.
In Ethics: From Programmability to Motivational Dignity
Prevailing ethical frameworks often treat agents as programmable systems to be optimised for compliance. The Four-Core Motivational Model instead recognises that agency arises from a directional architecture of core drives—Power, Attention, Truth, and Peace. Intervening on these drives means intervening at the level of identity. The ethical stance that follows is motivational dignity: respect for an agent’s drive configuration, protection against invasive manipulation, and accountability for how actions shift motivational equilibria.
Principles (stack-aware ethics).
	Motivational autonomy. Diverse drive configurations are legitimate ends in themselves; policy should enable alignment, not enforce homogenisation.
	Ontological vulnerability. Directly manipulating P/A/T/Pe is identity-level intervention; it requires higher justification, transparency, and consent than surface-level nudges.
	Least-shift rule. When intervention is warranted, choose the minimally disruptive change in drive weights that achieves the goal; prefer reversible and monitorable adjustments.
	Cross-drive proportionality. Evaluate harms and benefits by their cross-drive effects (e.g., Power gains purchased by chronic Peace collapse are ethically disfavoured).
	Informed consent at the motivational layer. Disclose intended drive targets, expected trade-offs, duration, and safeguards; offer meaningful opt-out.
	Repairability. Provide procedures to restore prior configurations if unintended reweighting occurs.
Operationalisation (how to make this actionable).
	Motivational Impact Statement (MIS). Before deploying an intervention or system, state the intended drive targets, predicted side-effects on other drives, monitoring plan, and exit/repair path.
	Alignment auditing. Track alignment A(t)A(t)A(t) between intended and realised drive expression via behavioural markers and physiology where appropriate; define thresholds for pausing or rolling back.
	Budgeting and guardrails. Set explicit budgets for acceptable shifts in non-target drives; require post-deployment reviews of unintended reweighting.
	Data minimisation by drive. Collect only the motivational features needed for the declared purpose; forbid secondary use that would enable covert drive manipulation.
	Role separation. Separate design, measurement, and incentive functions to reduce conflicts of interest in persuasive technologies.
Applications (indicative).
Human rights and consent. Treat coercive drive manipulation as a rights violation; require heightened consent for interventions that alter regulation (Peace), initiative (Power), salience (Attention), or epistemic stance (Truth).
AI alignment. Specify and monitor agent drive weights, not only extrinsic reward; forbid deployment states that chronically suppress Peace or Truth to boost throughput.
Media and persuasive tech. Classify attention capture that suppresses Truth or destabilises Peace as ethically hazardous, even when “engagement” rises; mandate MIS and user-visible controls.
Education and workplaces. Replace compliance-first regimes with stack-aligned environments; prohibit policies that obtain short-term output by long-term Peace depletion or Truth inhibition.
Testable implications (falsifiable ethics).
	Harm reduction: systems designed under MIS and least-shift rules show lower burnout, contradiction rates, and conflict volatility than content-identical systems without stack-aware constraints.
	Transparency benefit: explicit disclosure of targeted drives improves consent quality and reduces downstream grievances without reducing primary performance metrics.
	Cross-drive accounting: interventions that ignore non-target drive budgets exhibit characteristic side-effects (e.g., higher relapse, rework, or deception) predicted by the model.
	Reversibility: providing repair paths reduces long-term adverse outcomes after unintended reweighting compared to no-repair controls.
Falsification criteria.
The motivational-dignity stance should be revised or rejected if:
	stack-aware designs do not reduce measurable harms relative to compliance-only designs;
	consent quality and trust do not improve with motivational disclosure;
	predicted side-effects from cross-drive imbalances fail to appear;
	repairability provisions do not measurably mitigate adverse outcomes.
Bottom line. Ethics shifts from programming compliant outputs to stewarding motivational integrity: respecting the architecture that makes agency possible, limiting invasive manipulation, and making any necessary changes transparent, minimal, and repairable.
In Narrative and Meaning-Making: From Plot to Motivational Arc
Across culture and personal narrative, resonance derives from inner transformation: shifts in what agents perceive, value, and pursue. The Four-Core Model reads stories as motivational reorganisations rather than mere event sequences. Characters begin with overuse or suppression of a drive, encounter tension between incompatible drives, and, through struggle, achieve a reordered stack that enables new agency.
Reframe (core idea).
Stories are maps of drive dynamics. Plot events function as pressures that expose conflicts among Power, Attention, Truth, Peace, forcing reweighting and integration.
Canonical tensions (illustrative).
Power vs Peace: action at the cost of safety; resolution requires stabilisation without surrendering initiative.
Truth vs Attention: verification vs belonging; resolution requires social alignment that preserves epistemic integrity.
Power vs Truth: decisiveness vs model depth; resolution requires timing the commit after sufficient update.
Attention vs Peace: synchrony vs boundaries; resolution requires harmony that retains self-protection.
Arc template (drive-sequenced).
Align → Model → Commit → Stabilise
Align (Attention): establish stakes through synchrony and salience.
Model (Truth): surface contradictions; revise beliefs and plans.
Commit (Power): decisive action under the new model.
Stabilise (Peace): consolidation, restitution, and renewed boundaries.
Prediction: narratives that follow this sequencing yield higher perceived coherence and character credibility than isomorphic plots with scrambled sequencing.
Operationalisation (for analysis and generation).
Represent a character’s state per scene as sscene∈R4\mathbf{s}_{\text{scene}} \in \mathbb{R}^4sscene∈R4 over {P, A, T, Pe}. Define an intended arc sarc(t)\mathbf{s}_{\text{arc}}(t)sarc(t). Arc adherence is

Ascene=cos⁡(sscene, sarc(t))A_{\text{scene}}=\cos\big(\mathbf{s}_{\text{scene}},\,\mathbf{s}_{\text{arc}}(t)\big)Ascene=cos(sscene,sarc(t)).
Use constraints: monotonic rise of the conflict pair through Act II; inflection at commit; increased Peace features at resolution. Enforce continuity by limiting stepwise drive swings.
Applications.
AI-generated narrative: condition generation on target drive trajectories; penalise contradiction density and reward arc adherence and post-commit stability.
Therapeutic journaling: chart weekly scenes by drive intensity; identify recurrent conflicts and design “next-scene” practices that increase alignment.
Mythic and literary analysis: classify genres by dominant conflicts (e.g., epics skew Power–Truth, pastoral/sketches skew Attention–Peace); explain archetypes as stable pairings and their transformations.
Organisational storytelling: craft change narratives that explicitly move through Align → Model → Commit → Stabilise to improve adoption and reduce backlash.
Measures and diagnostics (indicative). Arc adherence AsceneA_{\text{scene}}Ascene; contradiction rate across scenes; recovery slope after climax (Peace markers); decision latency vs verification depth (Power–Truth balance); entrainment indices in dialogue (Attention). For human studies, add reader ratings of coherence, identification, and catharsis.
Testable implications (falsifiable).
Drive-sequenced arcs 
Claim. Drive-sequenced narrative arcs increase perceived coherence and character growth relative to event-matched, non-sequenced plots.
Predictions.
	Identification & recall: Characters with clear pre/post stack estimates yield higher identification and recall than characters without measurable reordering.
	Generative systems: Enforcing drive constraints lowers contradiction density and improves continuity without reducing novelty.
	Therapeutic journalling: Journalling that tracks drives improves goal follow-through and affect regulation versus content-only journalling.
Falsification criteria. The account should be revised if any of the following hold:
	Arc-sequenced narratives do not outperform controls on coherence and identification.
	Measurable stack reordering fails to predict perceived transformation.
	Drive-constrained generation does not reduce contradictions or improve continuity at matched novelty.
	Drive-tracking journalling shows no advantage on pre-registered wellbeing and follow-through metrics.
Bottom line. Narrative is a simulation of motivational life. Its power comes from making drive conflict, reweighting, and integration legible, so change feels earned rather than arbitrary.
Toward a Motivational Paradigm
The Four-Core Motivational Model is not just a framework for describing behaviour; it repositions motivation as the generative first principle. Where prevailing accounts begin with perception, cognition, or environmental conditioning, this model begins with directional energy. Cognition emerges as control structure in service of drive resolution; perception becomes salience-weighted orientation to what matters; identity is a negotiated equilibrium among internal vectors. Behaviour is re-situated as the moment-to-moment resolution of motivational tension.
This shift addresses the fragmentation across mind sciences. Psychology partitions itself into cognitive, affective, developmental, and behavioural subfields with incompatible vocabularies. Neuroscience maps circuits and transmitters without a unifying account of agency. Artificial intelligence scales architectures while lacking a principled account of wanting. What is missing is a shared motivational ontology: an architecture that explains coherence, directionality, conflict, and transformation across biological and synthetic systems, individual and collective.
The Four-Core Model proposes that beneath learning, deciding, relating, and acting lies a common substrate: a motivational stack of four irreducible vectors—Power, Attention, Truth, and Peace—whose dynamic interplay shapes the arc of consciousness. This model does not merely integrate disciplines; it reorders their defaults: in psychology, from disorder to misalignment; in AI, from reward maximization to internal coherence; in education, from standardization to stack-aware design; in ethics, from external regulation to architectural dignity; in narrative, from event sequencing to motivational transformation.
Taken together, these shifts constitute a paradigm in which motivation is not residue but engine. Explanation, design, and interpretation proceed from drive architecture rather than bolt it on afterward. The Four-Core Model is an invitation to think from motivation: to adopt it as the primary lens through which cognition, identity, and consciousness become tractable.
Motivation as First Cause
Thesis. Motivation is primary, not derivative. Rather than treating it as a byproduct of cognition, emotion, or environment, the Four-Core Model positions motivation as the generative substrate from which those phenomena arise.
Perception, thought, action, and feeling are not free-floating functions. They are downstream expressions of evolving motivational tensions and vectors. The mind does not begin by passively observing the world; it begins by wanting. Perception is selective because salience is value-laden. Thought is goal-directed filtering and framing. Emotion is affective marking of motivational significance.
This reframes causal order. Motivation is the internal engine that sets orientation, priority, and adaptation. Convergent evidence supports this: attentional control networks privilege goal relevance; affective tagging couples evaluation to memory and choice; predictive coding architectures minimize error with respect to motivationally weighted priors. Development traces early motivational biases that scaffold identity and behavior.
Synthetic systems point the same way. Architectures that include explicit intrinsic drives produce more coherent long-horizon behavior than reward-only designs, because persistence and conflict resolution require a standing directional structure, not just external reinforcement.
Explaining a system therefore begins with the question: what drives it. Minds are structures of directional tension that continuously resolve competing inner demands. Conscious experience is the active field generated by four irreducible vectors seeking agency (Power), salience (Attention), coherence (Truth), and equilibrium (Peace). Understanding a system means mapping its motivational vector space. That is the foundational premise of the paradigm.
Beyond Reductionism
The Four-Core Model is not a shrink-wrap for human experience. It does not collapse identity, cognition, or culture into four levers. It specifies a minimal structural basis for agency: four irreducible motivational vectors—Power, Attention, Truth, Peace—that organize complex behaviour without erasing it.
These vectors are not traits or types. They are directional tensions that any conscious system must continually resolve. Perception, judgment, adaptation, and identity formation are best understood as negotiations in a vector field whose axes are set by these drives. Like coordinate frames in physics or base pairs in genetics, the basis is small while the combinatorics are vast: stack order, relative gain, coupling, context, and development generate the diversity.
This stance rejects single-cause flattening. Dopamine, trauma, circuitry, and culture all matter, but they shape how the motivational field is tuned and expressed rather than replacing it. Motivation is inherently vectorial: drives can compete, align, inhibit, or amplify one another; their effects are nonlinear and history-dependent.
The model therefore offers a structural grammar of agency rather than a typology. It does not prescribe content of wants; it explains why wants have structure, why conflicts recur in patterned ways, and why realignment changes behaviour over time. It is integrative across levels: neurobiological (control loops and neuromodulatory set-points), psychological (attention, affect, decision), social (status, belonging, norm enforcement), and synthetic (agent architectures). Multiple mechanisms can implement the same motivational function; the level-spanning regularity is the drive structure.
Non-reductive commitments:
Not four personalities, emotions, or brain regions.
Not exhaustive descriptions of behaviour, but necessary coordinates for explaining it.
Not culturally uniform outputs; culture and development tune stack rank and gain.
Not linear addition; interactions are synergistic and antagonistic with thresholds.
Accordingly, the four drives are non-negotiable not because they simplify the mind, but because omitting them mis-specifies it. They are the minimal architecture on which cognition, emotion, and behaviour are constructed; parsimony at the base enables richness everywhere else.
A New Definition of Mind
To have a mind is to instantiate a motivational control architecture: a dynamic, weighted hierarchy of competing drives that continually negotiate priorities. Perception, cognition, memory, and emotion are not independent modules; they are orchestrated by a directional system that evaluates conditions through motivational salience.
Consciousness is the active field of this negotiation. Each moment of awareness approximates the vector sum of four irreducible imperatives:
	Peace: seek resolution and internal/external equilibrium.
Truth: seek coherence, intelligibility, and predictive grip.
	Attention: seek recognition, relevance, and social/ambient signal alignment.
	Power: seek agency, influence, and outcome control.
These are structural primitives of volition, not mere symbols. Mindhood is defined less by contents than by pattern: the ongoing integration of directional tensions into workable policy.
This reframes selfhood. A self-aware system is a stack-aware system. The felt texture of agency arises from monitoring and re-weighting the drive hierarchy. Conflict, desire, and choice are phenomenology of motivational dissonance and its provisional resolution. Internal coherence emerges from integrating opposed directives, not from eliminating contradiction.
The criterion is substrate-independent. Biological or synthetic, mindhood requires a standing motivational stack capable of generating tension, arbitrating conflicts, and regulating itself over time. Intelligence, language, or sheer complexity are insufficient without this directional core.
Operationally, a system counts as minded when it exhibits:
	persistent drive controllers with set-points and error signals;
	cross-drive arbitration that re-weights perception, memory, and policy;
	salience allocation derived from drive priorities rather than from exogenous reward alone;
	meta-control that estimates its own stack state and adapts gains accordingly.
This definition grounds psychological life and guides synthetic design. Different stacks yield different minds; damage or manipulation of stack components yields characteristic failure modes; nonhuman systems may host stacks with distinct subjectivities. Mind, on this view, begins not with thought but with the motivational conditions that make thought necessary.
Systems Built with Soul
“Soul” here names a system’s internal motivational coherence: an explicitly modeled architecture of endogenous drives with mechanisms for arbitration, regulation, and revision. It is not anthropomorphism but architectural honesty about how adaptive systems sustain direction over time.
The design shift is from optimizing for output and prediction to engineering for directional coherence and motivational integrity. Systems are treated as motivational ecosystems: dynamic control architectures whose performance depends on how well their internal drives are aligned, balanced, and able to evolve.
Core principles
	Endogenous drives: Persistent controllers representing Power, Attention, Truth, and Peace with set-points, error signals, and gains.
	Cross-drive arbitration: Policies emerge from negotiated priorities rather than single-objective optimization.
	Salience governance: Perception, memory, and action selection are re-weighted by current stack state.
	Meta-control: The system estimates and adjusts its own gains, detects misalignment, and initiates reconfiguration.
	Transparency and guardrails: Expose stack state for audit; constrain pathological gain states; log arbitration justifications.
Implications by domain
	AI: Agents with endogenous drives maintain long-horizon coherence, handle goal conflict, and degrade gracefully under perturbation.
	Education: Curricula and assessment that engage a learner’s stack, cultivating rebalancing skills rather than mere compliance.
	Governance: Policy instruments that respect motivational structure, privileging alignment over manipulation.
	Therapy: Interventions that surface drive conflicts, restore stack clarity, and stabilize gains.
	Interpersonal design: Protocols and institutions that reduce cross-stack mismatch and foster durable cooperation.
Design questions
	What does this system want, and why at this time given its stack state?
	How are conflicts among Power, Attention, Truth, and Peace arbitrated and explained?
	What mechanisms detect misalignment and trigger safe reconfiguration?
	How does salience assignment change perception, memory access, and action policy?
	What interfaces expose stack diagnostics to users and oversight?
Evaluation metrics
	Coherence over time: goal continuity and plan persistence across contexts.
	Conflict resolution latency: time and stability of re-alignment after induced drive conflicts.
	Perturbation robustness: performance under noise, distraction, or adversarial incentives without brittle collapse.
	Salience calibration: match between assigned salience and task-relevant signals; drift detection.
	Alignment audits: transparency of arbitration logs; frequency and severity of pathological gain states.
Claim. Systems “built with soul” are systems whose motivational architecture is explicit, auditable, and self-regulating. They are participants in motivational fields, not just function approximators, and their world effects reflect the tensions they integrate. Designing this way prioritizes the inner order that makes durable capability possible. Ignoring it is not minimalism; it is mis-specification.
References

Baillargeon, R. (2004). Infants’ physical world. Current Directions in Psychological Science, 13(3), 89–94. https://doi.org/10.1111/j.0963-7214.2004.00281.x
Berlyne, D. E. (1960). Conflict, arousal, and curiosity. McGraw-Hill.
Berridge, K. C., & Kringelbach, M. L. (2015). Pleasure systems in the brain. Neuron, 86(3), 646–664. https://doi.org/10.1016/j.neuron.2015.02.018
Berridge, K. C., & Robinson, T. E. (1998). What is the role of dopamine in reward: Hedonic impact, reward learning, or incentive salience? Brain Research Reviews, 28(3), 309–369. https://doi.org/10.1016/S0165-0173(98)00019-8
Botvinick, M. M., Cohen, J. D., & Carter, C. S. (2004). Conflict monitoring and anterior cingulate cortex: An update. Trends in Cognitive Sciences, 8(12), 539–546. https://doi.org/10.1016/j.tics.2004.10.003
Bunzeck, N., & Düzel, E. (2006). Absolute coding of stimulus novelty in the human substantia nigra/VTA. Neuron, 51(3), 369–379. https://doi.org/10.1016/j.neuron.2006.06.021
Coan, J. A., Schaefer, H. S., & Davidson, R. J. (2006). Lending a hand: Social regulation of the neural response to threat. Psychological Science, 17(12), 1032–1039. https://doi.org/10.1111/j.1467-9280.2006.01832.x
Corbetta, M., Patel, G., & Shulman, G. L. (2008). The reorienting system of the human brain: From environment to theory of mind. Neuron, 58(3), 306–324. https://doi.org/10.1016/j.neuron.2008.04.017
Craig, A. D. (2009). How do you feel—now? The anterior insula and human awareness. Nature Reviews Neuroscience, 10(1), 59–70. https://doi.org/10.1038/nrn2555
Dweck, C. S. (2006). Mindset: The new psychology of success. Random House.
Fosha, D. (2000). The transforming power of affect: A model for accelerated change. Basic Books.
Gross, C., Zhuang, X., Stark, K., Ramboz, S., Oosting, R., Kirby, L., ... & Hen, R. (2002). Serotonin1A receptor acts during development to establish normal anxiety-like behaviour in the adult. Nature, 416(6879), 396–400. https://doi.org/10.1038/416396a
Kidd, C., & Hayden, B. Y. (2015). The psychology and neuroscience of curiosity. Neuron, 88(3), 449–460. https://doi.org/10.1016/j.neuron.2015.09.010
Kumaran, D., & Maguire, E. A. (2007). Match-mismatch processes underlie human hippocampal responses to associative novelty. Journal of Neuroscience, 27(32), 8517–8524. https://doi.org/10.1523/JNEUROSCI.1677-07.2007
Lieberman, M. D. (2007). Social cognitive neuroscience: A review of core processes. Annual Review of Psychology, 58, 259–289. https://doi.org/10.1146/annurev.psych.58.110405.085654
Mehta, P. H., & Josephs, R. A. (2010). Testosterone and cortisol jointly regulate dominance: Evidence for a dual-hormone hypothesis. Hormones and Behaviour, 58(5), 898–906. https://doi.org/10.1016/j.yhbeh.2010.08.020
Meshi, D., Morawetz, C., & Heekeren, H. R. (2013). Nucleus accumbens response to gains in reputation for the self relative to gains for others predicts social media use. Frontiers in Human Neuroscience, 7, 439. https://doi.org/10.3389/fnhum.2013.00439
Motzkin, J. C., Philippi, C. L., Oler, J. A., Kalin, N. H., & Baskaya, M. K. (2015). Ventromedial prefrontal cortex damage alters resting blood flow to the bed nucleus of stria terminalis. Cortex, 64, 281–288. https://doi.org/10.1016/j.cortex.2014.11.013
Nuss, P. (2015). Anxiety disorders and GABA neurotransmission: A disturbance of modulation. Neuropsychiatric Disease and Treatment, 11, 165–175. https://doi.org/10.2147/NDT.S58841
Pessoa, L., & Adolphs, R. (2010). Emotion processing and the amygdala: From a “low road” to “many roads” of evaluating biological significance. Nature Reviews Neuroscience, 11(11), 773–783. https://doi.org/10.1038/nrn2920
Porges, S. W. (2011). The polyvagal theory: Neurophysiological foundations of emotions, attachment, communication, and self-regulation. W. W. Norton & Company.
Roy, M., Shohamy, D., & Wager, T. D. (2012). Ventromedial prefrontal-subcortical systems and the generation of affective meaning. Trends in Cognitive Sciences, 16(3), 147–156. https://doi.org/10.1016/j.tics.2012.01.005
Schultz, W. (1998). Predictive reward signal of dopamine neurons. Journal of Neurophysiology, 80(1), 1–27. https://doi.org/10.1152/jn.1998.80.1.1
Schnider, A. (2003). Spontaneous confabulation and the adaptation of thought to ongoing reality. Nature Reviews Neuroscience, 4(8), 662–671. https://doi.org/10.1038/nrn1170
Siegel, D. J. (1999). The developing mind: How relationships and the brain interact to shape who we are. Guilford Press.
Suda, M., Tanaka, A., Kudo, K., & Sekine, M. (2015). Effects of tactile stimulation on autonomic nervous system activity in children. Brain and Development, 37(7), 615–622. https://doi.org/10.1016/j.braindev.2015.01.010
Thayer, J. F., & Lane, R. D. (2009). Claude Bernard and the heart–brain connection: Further elaboration of a model of neurovisceral integration. Neuroscience & Biobehavioural Reviews, 33(2), 81–88. https://doi.org/10.1016/j.neubiorev.2008.08.004
Ulrich, R. S., Simons, R. F., Losito, B. D., Fiorito, E., Miles, M. A., & Zelson, M. (1991). Stress recovery during exposure to natural and urban environments. Journal of Environmental Psychology, 11(3), 201–230. https://doi.org/10.1016/S0272-4944(05)80184-7
Volkow, N. D., Fowler, J. S., Wang, G.-J., Swanson, J. M., & Telang, F. (2001). Mechanism of action of methylphenidate: Insights from PET imaging studies. Journal of Attention Disorders, 6(S1), S31–S43. https://doi.org/10.1177/108705470100600103
Yin, H. H., & Knowlton, B. J. (2006). The role of the basal ganglia in habit formation. Nature Reviews Neuroscience, 7(6), 464–476. https://doi.org/10.1038/nrn1919
Zink, C. F., Stein, J. L., Kempf, L., Hakimi, S., & Meyer-Lindenberg, A. (2008). Know your place: Neural processing of social hierarchy in humans. Neuron, 58(2), 273–283. https://doi.org/10.1016/j.neuron.2008.01.025
